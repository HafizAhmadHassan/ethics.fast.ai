{
  "00:00": "and I'm gonna start by going through a",
  "00:01": "few different case studies of bias to",
  "00:04": "kind of ground what we're talking and in",
  "00:05": "some practical real-world examples and",
  "00:08": "these also will illustrate some of the",
  "00:11": "different types of bias from the Hirini",
  "00:14": "Suresh and John Guttag paper and the",
  "00:18": "first is gender shades which I would",
  "00:20": "imagine many people are familiar with",
  "00:22": "this has received a lot of a lot of",
  "00:25": "media attention which is good and this",
  "00:27": "was a study from joy ballon weeny",
  "00:29": "intimidate gabru evaluating commercial",
  "00:32": "computer vision products from Microsoft",
  "00:35": "IBM and face plus plus and then they did",
  "00:37": "a follow-up study that looked at Amazon",
  "00:39": "Kairos and a few other companies and",
  "00:42": "they kind of consistently found that",
  "00:45": "these news classifiers had significantly",
  "00:49": "worse performance on dark skinned women",
  "00:51": "and here it's really significant that",
  "00:54": "they didn't just look at light skinned",
  "00:56": "versus dark skinned or women versus men",
  "00:59": "but kind of broke it out into",
  "01:01": "subcategories to kind of capture that",
  "01:04": "for instance IBM's product was ninety",
  "01:07": "nine point seven percent accurate on",
  "01:08": "light skinned men and then only sixty",
  "01:11": "five point three percent accurate on",
  "01:12": "dark skinned women and again this these",
  "01:15": "were commercial products that had been",
  "01:17": "released",
  "01:21": "so yeah the follow-up found found",
  "01:24": "something similar with Amazon a separate",
  "01:27": "study from the ACLU found that Amazon's",
  "01:30": "facial recognition in correctly matched",
  "01:32": "twenty eight members of Congress to",
  "01:34": "criminal mug shots and this",
  "01:37": "disproportionately included people of",
  "01:39": "color who are wrongly matched and this",
  "01:43": "technology is already in use even though",
  "01:45": "it's virtually unregulated and so this",
  "01:50": "is this is I think pretty concerning and",
  "01:52": "alarming and so I like I like this",
  "01:57": "really like this paper that I assigned",
  "01:59": "on a framework for understanding",
  "02:03": "unintended consequences of machine",
  "02:05": "learning because it kind of breaks down",
  "02:07": "bias into different sources have",
  "02:09": "different causes which is helpful for",
  "02:12": "addressing it",
  "02:13": "and that kind of pretty quickly you know",
  "02:16": "it's helpful to to know that bias is an",
  "02:18": "issue and exist but you kind of need to",
  "02:20": "to move beyond that surface level",
  "02:23": "understanding to think about how how to",
  "02:25": "address it and where it's coming from so",
  "02:28": "in this case this is representation bias",
  "02:30": "where the kind of data that you've",
  "02:34": "trained on is not representative of the",
  "02:36": "data that this is being deployed on",
  "02:40": "however because this was a problem kind",
  "02:42": "of not just for one company it's also an",
  "02:46": "example of evaluation bias and so in",
  "02:50": "machine learning benchmark data sets per",
  "02:52": "on a lot of research which in some ways",
  "02:54": "is a positive but it also means that",
  "02:57": "biases in those benchmark data sets end",
  "03:00": "up kind of being replicated at scale and",
  "03:02": "that you have like a whole body of",
  "03:04": "research kind of on top of those biases",
  "03:08": "and so a lot of the popular data sets of",
  "03:13": "faces primarily included light-skinned",
  "03:15": "men up until kind of just in the last",
  "03:18": "year or two where people are starting to",
  "03:19": "address this and so for instance I GPA",
  "03:22": "was a data set of faces and only four",
  "03:26": "percent of the images or of dark-skinned",
  "03:28": "women we also see this with image net",
  "03:32": "which is I think probably the best",
  "03:34": "studied computer vision set out there",
  "03:36": "two-thirds of image net images are from",
  "03:38": "the West and so an image net is for kind",
  "03:43": "of classifying",
  "03:44": "all sorts of different you know types of",
  "03:46": "animals and household appliances and",
  "03:48": "vehicles and so it's not not people for",
  "03:51": "for the most part but kind of all these",
  "03:53": "different things but which look",
  "03:55": "different in different cultures and so",
  "03:58": "as a result there are higher error rates",
  "04:00": "kind of a few one example that is of a",
  "04:03": "person is bridegroom so me I'm getting",
  "04:06": "married and this study found that there",
  "04:08": "was a much higher error rate identifying",
  "04:11": "bridegroom's from like Egypt and India",
  "04:14": "and countries outside the West",
  "04:19": "and so a way to address kind of",
  "04:22": "representation data is creating more",
  "04:24": "representative datasets and joy",
  "04:28": "intimidate did that as part of gender",
  "04:30": "shades it's really important to keep in",
  "04:32": "mind consent when building datasets and",
  "04:35": "not taking people's images without their",
  "04:37": "consent and so this was a well-built",
  "04:39": "data set but there are examples that",
  "04:41": "were not and I will I will come back",
  "04:44": "because this is not the full answer and",
  "04:47": "there's more more to this so we'll kind",
  "04:49": "of return to this example later any any",
  "04:54": "questions on on that Oh Lauren and who",
  "04:57": "has the catch box where especially with",
  "05:15": "my Covina nonprofit challenge sources is",
  "05:25": "there way like ask a go whore own",
  "05:28": "organizations",
  "05:33": "that's it that's a good question I'll",
  "05:35": "get some I have a section on kind of",
  "05:37": "towards solutions later on but the kind",
  "05:40": "of brief answer and I actually don't",
  "05:42": "know about Hagler I'm Dena in particular",
  "05:44": "but kind of being very thoughtful about",
  "05:48": "how you construct the data set which is",
  "05:50": "in general machine learning has often",
  "05:54": "kind of just focused on gathering huge",
  "05:57": "amounts of data efficiently and not as",
  "05:59": "much about kind of how that data is",
  "06:00": "structured or what sort of risk might be",
  "06:02": "in there but I will come back to that",
  "06:04": "into the later section any other",
  "06:06": "questions",
  "06:07": "okay further down the fourth row",
  "06:15": "it would seem to me that as the",
  "06:17": "population that it's being used on",
  "06:20": "changes we should be adjusting the",
  "06:22": "Concord historical data to match the",
  "06:25": "populations as its throwing and changing",
  "06:28": "do we see that or how could that be I",
  "06:32": "mean so it depends how your populations",
  "06:36": "changing it maybe it depends on that",
  "06:39": "also the type of data may be hard to",
  "06:40": "change the date I like I think it's",
  "06:42": "easier to realize that your data is no",
  "06:44": "longer representative of what you need",
  "06:45": "in many cases then to actually generate",
  "06:48": "the data that would be more",
  "06:49": "representative but we will talk about it",
  "06:53": "because I think that relates some to",
  "06:54": "historical bias which I'll talk about in",
  "06:56": "a moment",
  "06:59": "all right next next step is and the",
  "07:04": "compass recidivism algorithm used in",
  "07:06": "prison sentencing this is another kind",
  "07:08": "of very famous and well studied example",
  "07:11": "but in 2016 Pro Publica did an",
  "07:14": "investigation real inspecting the",
  "07:17": "software that is sold by a for-profit",
  "07:19": "company it's in use in many states in",
  "07:23": "the US and it can be used for pretrial",
  "07:26": "decisions so this is deciding who has to",
  "07:29": "pay bail prior to even having a trial",
  "07:33": "and many people in the US are in jail",
  "07:35": "because they're too poor to afford bail",
  "07:37": "it's also used in sentencing decisions",
  "07:40": "and in parole decisions so having a very",
  "07:43": "significant impact on people's lives and",
  "07:45": "what they found is looking at kind of a",
  "07:49": "county in Florida that the false",
  "07:50": "positive rate for black defendants was",
  "07:54": "almost twice as high as for white",
  "07:57": "defendants and so this is people being",
  "07:59": "labeled as high risk of committing",
  "08:01": "another crime or of being really",
  "08:04": "arrested and the false positive rate was",
  "08:06": "forty five percent which is also just a",
  "08:08": "really terribly high well it's positive",
  "08:11": "rate when you're labeling people as just",
  "08:14": "a binary high risk or low risk there was",
  "08:18": "a study from Dartmouth that came out a I",
  "08:22": "remember a year or two ago that found",
  "08:24": "that the software is no more accurate",
  "08:26": "than Amazon Mechanical Turk workers",
  "08:29": "so random people on the internet we're",
  "08:31": "kind of just as accurate and their",
  "08:33": "judgments of whether someone was high",
  "08:34": "risk or low risk another another thing",
  "08:37": "this Dartmouth study found was that so",
  "08:41": "the compass recidivism software is a",
  "08:43": "black box that takes it see there 130 or",
  "08:46": "140 inputs and it was not any more",
  "08:50": "accurate than a linear classifier on",
  "08:52": "three variables so it's also just not",
  "08:55": "not particularly accurate yet the",
  "08:57": "Wisconsin Supreme Court upheld its use",
  "09:00": "and it's used in states other than",
  "09:02": "Wisconsin but this is one place where it",
  "09:04": "was challenged and this is a kind of a",
  "09:08": "in Iran Iran Yuans twenty-one",
  "09:11": "definitions of fairness talk a lot of",
  "09:13": "that is kind of based off of kind of",
  "09:15": "going deeper around around compass and",
  "09:17": "it is true that the company was using",
  "09:19": "this different definition of fairness",
  "09:21": "that is not about false positives that",
  "09:24": "it does satisfy and so one one key thing",
  "09:32": "to really highlight is that race is not",
  "09:36": "an input to the software and so even",
  "09:38": "though there was this huge discrepancy",
  "09:40": "in the false positive rates race was not",
  "09:43": "an explicit input and it's important to",
  "09:46": "remember that machine learning is kind",
  "09:48": "of largely about identifying latent",
  "09:50": "variables which is you know a variable",
  "09:52": "you haven't explicitly named or recorded",
  "09:55": "in your data set and so you can still",
  "09:57": "have these kind of hugely different",
  "10:01": "results on different groups even when",
  "10:03": "you're not using that input and this is",
  "10:04": "a really common misconception a lot of",
  "10:08": "companies when they're accused of bias",
  "10:10": "are like no you know we swear we're not",
  "10:12": "using gender as an input or we're not",
  "10:13": "using races and input but you can still",
  "10:16": "be very biased and have very different",
  "10:17": "results even if you're not questions on",
  "10:23": "that",
  "10:28": "all right this is also an example of a",
  "10:32": "feedback loop which we talked about a",
  "10:33": "little bit last week in the context of",
  "10:35": "recommendation systems and a feedback",
  "10:38": "loop is kind of whenever your model is",
  "10:41": "controlling the next round of data you",
  "10:42": "get and the data that is returned",
  "10:45": "quickly becomes flawed by the software",
  "10:47": "itself so in the case of predictive",
  "10:50": "policing so this is a slightly different",
  "10:51": "case but where you're trying to predict",
  "10:54": "which neighborhoods are gonna have the",
  "10:55": "most crime so you can send more police",
  "10:57": "to those neighborhoods having more",
  "10:59": "police in a neighborhood may also result",
  "11:02": "in more arrests just because they're",
  "11:03": "more police around to to see something",
  "11:06": "or to arrest someone which could then",
  "11:07": "you know feed back into your algorithm",
  "11:09": "and say let's send even more police to",
  "11:11": "that neighborhood one researcher who",
  "11:15": "works on this suresh Pinkett TAS",
  "11:17": "Subramanian says predictive policing is",
  "11:20": "aptly named it is predicting future",
  "11:22": "policing not future crime as I liked",
  "11:25": "liked that quote as an example of kind",
  "11:28": "of how how a feedback loop can happen",
  "11:34": "and this this is an example of",
  "11:38": "historical bias and historical bias is a",
  "11:41": "fundamental structural issue with the",
  "11:43": "first step of the data generation",
  "11:45": "process and can exist even given perfect",
  "11:47": "sampling and feature selection so this",
  "11:50": "is a case where going out and saying",
  "11:52": "like oh let's get crime data for more US",
  "11:56": "Jewish York jurisdictions is not",
  "11:58": "necessarily going to remove the it's not",
  "12:00": "gonna remove the racial bias because",
  "12:02": "this is really kind of present in the",
  "12:04": "data and it's really true because of",
  "12:06": "because of our history and unfortunately",
  "12:08": "historical bias is I think very hard to",
  "12:11": "address and it's also a very common type",
  "12:13": "of bias yes and who has the catch box",
  "12:17": "okay pass it to the front so I might not",
  "12:22": "be phrasing this question accurately but",
  "12:24": "I'm curious about how it seems like the",
  "12:28": "biases that's come up in these examples",
  "12:31": "are most frequently race and gender",
  "12:32": "which are characteristics their physical",
  "12:35": "attributes are their biases that have",
  "12:38": "shown up",
  "12:40": "um that are less physical but still I",
  "12:43": "guess I'm trying to ask around the",
  "12:45": "latent variables are there things that",
  "12:47": "have shown up that are not necessarily",
  "12:48": "physical in a data set I am sure that",
  "12:52": "there are because it like in machine",
  "12:54": "learning like whatever you're trying to",
  "12:55": "predict it's you can describe as a",
  "12:58": "latent variable because that's something",
  "12:59": "that's not in the data set I think why",
  "13:01": "so many of the examples on bias are",
  "13:03": "around race and gender just because",
  "13:05": "they're easy to easy er to spot when",
  "13:08": "they happen whereas some things and I",
  "13:15": "mean you can I guess like education or",
  "13:18": "socioeconomic status or country of",
  "13:20": "origin or language there has been some",
  "13:23": "some research around language so yeah",
  "13:26": "bias definitely exists on other",
  "13:28": "variables and then things that are also",
  "13:31": "more seemingly neutral I don't know if",
  "13:34": "you were identifying species of a tree",
  "13:36": "or something like you could have bias",
  "13:39": "but in this context we're kind of",
  "13:40": "talking about I should I should have",
  "13:42": "made that clear bias has multiple",
  "13:43": "definitions which can be confusing and",
  "13:45": "so here I'm not talking about the",
  "13:47": "statistical term but I'm talking about",
  "13:49": "kind of unjust bias arguably like race",
  "14:03": "and gender are actually socially",
  "14:05": "constructed and they're not really",
  "14:07": "representative of an exact biological",
  "14:10": "thing in particular when you think about",
  "14:11": "like what is white what has qualified as",
  "14:14": "white has changed over the years and",
  "14:17": "hundred years ago people that we would",
  "14:19": "now consider to be quiet",
  "14:20": "based on their country of origin or",
  "14:22": "whatever would not have been considered",
  "14:24": "that's so like it there is actually this",
  "14:26": "strongly is socially constructed and I",
  "14:28": "realize that's kind of a cop-out to your",
  "14:29": "question but is sort of worth I guess",
  "14:32": "like you know to say like oh actually a",
  "14:34": "lot of the stuff that it's picking up on",
  "14:36": "are things that we've socially",
  "14:38": "instructed already yeah yeah that's a",
  "14:42": "good point",
  "14:45": "maybe someday we like to add that it's",
  "14:47": "like I think basically this bias things",
  "14:50": "that people is intentionally growing for",
  "14:52": "right",
  "14:53": "so it's not like somehow you have the",
  "14:55": "model and the model will tell you can I",
  "14:58": "wear this tie as poor as people cannot",
  "14:59": "breaking up this laptop these categories",
  "15:01": "and then finding this imbalance right so",
  "15:04": "for sure probably there's bias even many",
  "15:07": "in spy squad people can I will be",
  "15:10": "willing to check for okay interspace or",
  "15:12": "not right and yeah and I don't know",
  "15:17": "really about probably there's some ways",
  "15:19": "to try to do some unsupervised manner",
  "15:21": "where you see someone in a shoes I'm",
  "15:24": "trying to find clusters and then you had",
  "15:26": "a gig in which that the phone is",
  "15:28": "associated those classic no way and then",
  "15:30": "you do the other way around but yeah",
  "15:32": "there is a paper on that topic I can",
  "15:34": "find the link of because like Elise said",
  "15:37": "because these are kind of are these are",
  "15:38": "socially constructed categories there is",
  "15:41": "something that can be you know if he",
  "15:44": "about like assigning I think this person",
  "15:47": "is this race or something around this",
  "15:49": "you know socially constructed category",
  "15:51": "and so there there was a paper last year",
  "15:53": "I know I'm trying to unstructured",
  "15:56": "approach in an unstructured way and look",
  "15:58": "looking for categories there's also a",
  "16:00": "talk from our tech policy workshop and",
  "16:03": "we'll be hopefully releasing those in",
  "16:04": "the next month that Christiane LOM who",
  "16:07": "works for the human rights data analysis",
  "16:09": "group and she does a lot of work with",
  "16:11": "kind of a how data is used in our",
  "16:14": "criminal justice system and she kind of",
  "16:18": "talked about inspecting this a lot of",
  "16:20": "data from police it's just the police",
  "16:22": "are assigning what race that they think",
  "16:24": "the person is without asking them and so",
  "16:27": "you know they're like there's I mean",
  "16:28": "this is like a kind of problem with the",
  "16:30": "data source but then it's also like",
  "16:32": "that's the data we have to look at so",
  "16:35": "it's definitely imperfect and there are",
  "16:38": "kind of issues around it although I do I",
  "16:42": "do think it is like you know you want to",
  "16:44": "be looking for racial bias and so",
  "16:46": "sometimes you are having to kind of work",
  "16:47": "with the data you have even when it's",
  "16:50": "going to contain inaccuracies and is",
  "16:52": "around this yeah kind of socially",
  "16:54": "constructed categorization",
  "17:07": "ktd discovery databases I didn't know",
  "17:10": "around since 1989 and I was thinking",
  "17:13": "about like why only now is ethics coming",
  "17:15": "up as an issue even though we've had",
  "17:16": "this you know sort of like practitioners",
  "17:19": "in the field who think considering in",
  "17:20": "this is it only because I don't know an",
  "17:23": "overreaction from like the 2016 election",
  "17:25": "is it because we're understanding mom",
  "17:29": "more deeply now is she learning and",
  "17:30": "practice is biased so I was briefly it's",
  "17:35": "so I think there were people like there",
  "17:37": "are people that have been considering",
  "17:38": "ethics for longer and often in these",
  "17:40": "kind of adjacent fields of like STS or",
  "17:44": "library science I think have more of a",
  "17:47": "history of thinking about ethics but I",
  "17:50": "think right now what we've seen is kind",
  "17:52": "of the explosion of these of machine",
  "17:55": "learning a different algorithm ated",
  "17:57": "decision systems being used much more",
  "17:59": "widely in practice and so it's I think a",
  "18:02": "lot of why we're seeing kind of more",
  "18:04": "ethical alarm is the the widespread use",
  "18:06": "and there is I will note that like many",
  "18:10": "many fields have had to deal with kind",
  "18:12": "of the ethical quandary of their field",
  "18:15": "and actually Alley has a great essay",
  "18:18": "that will read later on kind of",
  "18:19": "anthropology dealing with ethical issues",
  "18:21": "but people have talked about you know",
  "18:23": "physics and the atomic bomb and how that",
  "18:25": "raised ethical issues and so it's also",
  "18:28": "kind of not you know we are kind of",
  "18:30": "experiencing like a major kind of",
  "18:32": "reckoning for the field but we're not",
  "18:33": "the first field to go through that as",
  "18:35": "well well I'll keep moving we'll come",
  "18:38": "back to I think some kind of related",
  "18:39": "things on all of this as we go so what",
  "18:45": "and I'm here's no kind of one",
  "18:46": "recommendation of something you can do",
  "18:48": "towards towards trying to address this",
  "18:52": "and that is speaking with domain experts",
  "18:54": "and people impacted and so at the fat",
  "18:59": "star conference I think in 2019",
  "19:04": "Christian Lomb Elizabeth bender and",
  "19:07": "Terrance Wilkerson hosted a really great",
  "19:08": "workshop and it's available online and",
  "19:12": "so christian lamb is the statistician i",
  "19:15": "mentioned who works with human rights",
  "19:16": "data analysis group and then elizabeth",
  "19:19": "bender as a former",
  "19:20": "public defender and Terrence Wilkerson",
  "19:22": "is an innocent man who is wrongly",
  "19:25": "accused of of committing a crime and",
  "19:27": "couldn't afford bail and Elizabeth and",
  "19:30": "Terrence were able to give a lot of",
  "19:31": "insight into how the criminal justice",
  "19:33": "system works in practice and it's things",
  "19:35": "that I wouldn't necessarily know as a as",
  "19:39": "a computer scientist and coming from a",
  "19:41": "more technical background and it's",
  "19:43": "really important to get those kind of",
  "19:45": "implementation and human details of how",
  "19:48": "how things actually work so Elizabeth",
  "19:50": "talked about when she would go to visit",
  "19:53": "one of her defendants on Rikers Island",
  "19:56": "she was having to take a bus like two",
  "19:58": "hours each way and only getting like 30",
  "20:01": "minutes with the defendant if the guards",
  "20:03": "brought brought them in on time and kind",
  "20:05": "of all these limitations and Terence",
  "20:07": "talked about a lot of the just how kind",
  "20:09": "of terrible it is to be in prison and",
  "20:12": "the pressure to take a guilty plea",
  "20:15": "bargain and to get out quicker and also",
  "20:17": "how it can kind of impact how you appear",
  "20:20": "before the judge when you've been in",
  "20:22": "prison and so is a really helpful",
  "20:26": "session and those sorts of",
  "20:27": "collaborations of talking closely with",
  "20:30": "the people that have the domain",
  "20:31": "knowledge and are impacted is really",
  "20:33": "crucial and I think one step towards",
  "20:35": "towards trying to address this type of",
  "20:37": "bias or at least towards trying to",
  "20:40": "address the harms from it all right the",
  "20:42": "next so the third a third case study",
  "20:45": "I'll talk about is online ad delivery",
  "20:47": "and I shared this shared this last week",
  "20:50": "so we'll just briefly go go over it",
  "20:53": "again but we talked about Latanya",
  "20:54": "Sweeney computer science professor at",
  "20:56": "Harvard who was getting these ads saying",
  "20:59": "Latanya Sweeney arrested she had never",
  "21:02": "been arrested",
  "21:03": "she found names like Kirsten Lindquist",
  "21:06": "who has been arrested three times got",
  "21:07": "more neutral language just saying we",
  "21:10": "found Kirsten Lindquist",
  "21:11": "she studied over 2000 names and",
  "21:14": "confirmed this pattern the",
  "21:16": "disproportionately African American",
  "21:18": "names we're getting the ads suggesting",
  "21:21": "they'd been arrested and then this kind",
  "21:23": "of discrimination and advertising has",
  "21:26": "continued to show up in a lot of",
  "21:28": "different forms there was a paper last",
  "21:31": "year of even when advertisers",
  "21:33": "are trying to discriminate their ads get",
  "21:36": "shown to very different audiences and so",
  "21:39": "the researchers found that a job ad",
  "21:41": "depending on what the job was could be",
  "21:42": "shown to an audience of 90 percent men",
  "21:45": "or 90 percent women this is on Facebook",
  "21:48": "although I think this is true on many",
  "21:49": "platforms housing ads changing the",
  "21:54": "picture between a white family and a",
  "21:56": "black family they got very different",
  "21:59": "audiences even when the text was",
  "22:00": "identical and that this is this is an",
  "22:03": "issue because things like housing and",
  "22:06": "jobs really relate to kind of our civil",
  "22:08": "rights and this is just a series of",
  "22:13": "ProPublica is a pro public as fantastic",
  "22:16": "it does fantastic work but they found",
  "22:19": "that facebook dozens of companies were",
  "22:22": "using Facebook to only show job ads to",
  "22:24": "young people",
  "22:27": "Facebook was letting was also letting",
  "22:30": "housing advertisers explicitly exclude",
  "22:32": "users by race and actually should update",
  "22:35": "this so there was an initial this",
  "22:37": "discovery in 2016 that they were doing",
  "22:39": "this and Facebook apologized and then",
  "22:41": "over a year later they were still doing",
  "22:43": "this and so as you could place a housing",
  "22:45": "ad and say you know I don't want to show",
  "22:47": "this to Latin X people or I don't want",
  "22:51": "to show it to there was like you I don't",
  "22:52": "want to show this to wheelchair users",
  "22:53": "and the kind of very very discriminatory",
  "22:57": "so then they were still doing it over a",
  "22:59": "year later and then they did reach a",
  "23:01": "settlement last year and implemented",
  "23:04": "this new system that is supposed to",
  "23:06": "limit this and then ProPublica did",
  "23:08": "another investigation how their their",
  "23:10": "new platform is not not actually",
  "23:12": "effective and then we saw Amazon",
  "23:16": "matching members of Congress to criminal",
  "23:18": "mug shots there's also the Amazon resume",
  "23:22": "screening tool that penalized resumes",
  "23:25": "with the word women's and them so this",
  "23:27": "shows up lots of places kind of this",
  "23:29": "bias I mean so I do want to highlight",
  "23:31": "that many AI ethics concerns are about",
  "23:34": "human rights and civil rights",
  "23:36": "I like there's an article from a Neil -",
  "23:39": "where he says there is no tech industry",
  "23:42": "anymore tech is used in every industry",
  "23:44": "and so this term has kind of become",
  "23:46": "meaningless",
  "23:47": "it's important to really think about the",
  "23:49": "particular verticals that you're talking",
  "23:51": "about and so others have proposed and I",
  "23:55": "agree with kind of one one way to when",
  "23:57": "people talk about the idea of regulating",
  "23:59": "AI is to really think about kind of what",
  "24:01": "human rights and civil rights were",
  "24:02": "trying to protect in four different",
  "24:04": "particular areas whether that is housing",
  "24:06": "or education employment criminal justice",
  "24:08": "so going out are kind of going back to",
  "24:11": "this paper by Suresh and GU tagged this",
  "24:15": "concept of biased data has is often too",
  "24:17": "generic to really be useful and so",
  "24:21": "another another paper I like from Cindy",
  "24:26": "el milena --then and I had Overmeyer",
  "24:28": "looked at historical electronic health",
  "24:31": "record data and asked what factors are",
  "24:34": "most predictive of stroke and so they",
  "24:38": "found and they're saying you know this",
  "24:41": "could be useful in prioritizing patients",
  "24:43": "at the ER and so they found the number",
  "24:45": "one most predictive factor was having",
  "24:47": "had a prior stroke which totally makes",
  "24:49": "sense",
  "24:50": "second was cardiovascular disease which",
  "24:53": "also makes sense",
  "24:54": "and then next was accidental injury a",
  "24:58": "benign breast lump colonoscopy and",
  "25:03": "sinusitis and so you don't have to be a",
  "25:06": "medical doctor to say why those",
  "25:08": "something seems wrong why would those",
  "25:10": "things be predictive of of having a",
  "25:14": "stroke",
  "25:16": "does anyone want to guess why up here oh",
  "25:23": "can someone patches past the catch box",
  "25:25": "forward so the guess was older person",
  "25:34": "that's not what they found but good",
  "25:36": "guess and then here oh okay over here",
  "25:51": "we are so close",
  "25:56": "so I'll what one more guess behind you",
  "25:59": "from Colin but then someone who wasn't",
  "26:02": "in the hospital for something else you",
  "26:04": "can found the bias no that's also a good",
  "26:08": "guy so I like how you're thinking and",
  "26:10": "this oh claudia has a guest in the back",
  "26:12": "that corner it could be part of their",
  "26:22": "health history that may be self",
  "26:24": "reporting yes that's it so it's well",
  "26:29": "they classify it as who utilizes health",
  "26:33": "care versus not so they're basically",
  "26:35": "people that utilize health care a lot",
  "26:36": "and people that don't and there are a",
  "26:38": "number of factors that can contribute to",
  "26:40": "this of who even has access to health",
  "26:42": "care who can afford their copay who can",
  "26:45": "take time off of work there may be",
  "26:47": "cultural factors there's racial and",
  "26:50": "gender bias and the types of treatment",
  "26:51": "that people receive and so actually I",
  "26:54": "guess this goes back to your earlier",
  "26:56": "question of picking up late and bias of",
  "26:58": "something they found here they're",
  "26:59": "picking up bias basically of high",
  "27:02": "utility versus low utility for health",
  "27:04": "care which is yeah not a physical",
  "27:06": "characteristic but is making a",
  "27:11": "significant difference and so if you're",
  "27:13": "someone who is gonna go to the ER when",
  "27:16": "when you have an accidental inter injury",
  "27:19": "or sinusitis then you also are gonna go",
  "27:21": "to the ER when you're having a stroke",
  "27:23": "and so they said you know we haven't we",
  "27:26": "haven't measured stroke we've measured",
  "27:27": "who had symptoms went to the doctor got",
  "27:30": "the correct test and then received the",
  "27:32": "diagnosis of stroke and so this is one",
  "27:35": "type of measurement bias and there are",
  "27:38": "there many kind of many types of",
  "27:40": "measurement bias but it's a way that",
  "27:42": "you're kind of measurements aren't",
  "27:43": "capturing exactly exactly what you think",
  "27:46": "they are and you know and even kind of",
  "27:49": "more biologically stroke is about region",
  "27:52": "of the brain being denied oxygen and",
  "27:54": "that's something we don't have we don't",
  "27:56": "even have the data for you know that'd",
  "27:57": "be like we would need everyone wearing",
  "27:58": "brain monitors to to know who has had",
  "28:01": "stroke we have these other things you",
  "28:03": "know in medicine",
  "28:04": "it often ends up being about kind of",
  "28:05": "like a billing code in a chart and so",
  "28:09": "this is yeah I think I kind of subtle",
  "28:11": "form of bias that that could show up a",
  "28:17": "question in the second row and this is",
  "28:21": "ed this is also another example we're",
  "28:23": "gathering more of the same type of data",
  "28:25": "you already have it's not going to help",
  "28:27": "you I mean if you're able to gather kind",
  "28:29": "of a different type of data that better",
  "28:30": "butter gets at your question but just",
  "28:33": "gathering more of the same won't help",
  "28:34": "here and then a fifth type of bias that",
  "28:39": "they talked about in the paper is",
  "28:40": "aggregation bias and they give the",
  "28:44": "example of diabetes patients having",
  "28:47": "different complications across",
  "28:48": "ethnicities and also kind of some of",
  "28:51": "these blood markers used to diagnose and",
  "28:54": "monitor diabetes differing across",
  "28:56": "ethnicities and gender and so this this",
  "28:59": "could combine with other biases if you",
  "29:01": "have so this suggests that maybe one one",
  "29:06": "model is not going to do a great job of",
  "29:08": "representing representing people from",
  "29:11": "different ethnicities or genders also if",
  "29:13": "your dataset had representation bias you",
  "29:16": "could then end up kind of drowning out a",
  "29:18": "certain group and you would have a",
  "29:19": "higher error rate for that group so this",
  "29:22": "is another another type of bias and so",
  "29:28": "they they look at five and the paper but",
  "29:32": "I think it's helpful to see that I think",
  "29:34": "what's most talked about of kind of",
  "29:36": "gathering a more diverse dataset is",
  "29:38": "really only going to help for",
  "29:40": "representation and evaluation bias but",
  "29:43": "most likely not the other three then I",
  "29:46": "want to talk a little bit more about",
  "29:47": "kind of where where racial bias is found",
  "29:51": "in all the studies I'm going to cite",
  "29:53": "here are from a New York Times article",
  "29:55": "that Cindy Ulm Elena thought and wrote",
  "29:57": "but these are all from kind of",
  "29:59": "peer-reviewed academic research and this",
  "30:03": "is just a sample of what's out there but",
  "30:06": "when doctors were shown identical files",
  "30:07": "they make they make are much less likely",
  "30:11": "to recommend a helpful cardiac procedure",
  "30:13": "to black patients than to white patients",
  "30:17": "when bargaining for a used car black",
  "30:19": "people were offered initial prices $700",
  "30:21": "higher and received faller far smaller",
  "30:23": "concessions responding to apartment",
  "30:27": "rental ads on Craigslist with a black",
  "30:29": "name elicited fewer responses than with",
  "30:31": "a white name an all-white jury with 16",
  "30:34": "points more likely to convict a black",
  "30:36": "defendant than a white one but when a",
  "30:37": "jury had one black member it convicted",
  "30:40": "both at the same rate and actually I",
  "30:43": "think I had more in shorten debt and so",
  "30:46": "I just say just share this to show that",
  "30:47": "kind of an a wide variety of types of",
  "30:50": "data whether you're looking at medical",
  "30:53": "data or sales data or housing data or",
  "30:56": "political data that it is if it involves",
  "30:59": "human it's quite humans it's quite",
  "31:01": "likely that there's racial bias in it",
  "31:02": "that this is very very pervasive however",
  "31:08": "this raises the question that I that I",
  "31:11": "hear sometimes which is that you know",
  "31:13": "humans are really biased this is well",
  "31:15": "documented so why kind of why is there",
  "31:18": "so much concern about algorithmic bias",
  "31:21": "given this that humans are so biased and",
  "31:26": "I have a kind of four reasons that I",
  "31:29": "think algorithmic bias still really",
  "31:30": "matters and the first is machine",
  "31:32": "learning can amplify bias so in many",
  "31:34": "cases it's not just encoding kind of",
  "31:37": "existing human bias but it can also be",
  "31:39": "amplifying the magnitude so this is some",
  "31:43": "research from Maria de Arteaga at all",
  "31:46": "that looked at LinkedIn kind of how",
  "31:50": "people describe their job description",
  "31:52": "and in their data set they described it",
  "31:57": "as imbalances being compounded only 14%",
  "32:00": "of the surgeons were female and then in",
  "32:03": "their true positives only 11% of the",
  "32:07": "surgeons were female and so basically",
  "32:09": "there's kind of this asymmetric a",
  "32:11": "symmetry around false negatives and",
  "32:15": "false positives when you have kind of a",
  "32:17": "very unbalanced data set to begin with",
  "32:22": "second a second reason that I think we",
  "32:25": "really need to take algorithmic bias",
  "32:27": "seriously is because algorithms are",
  "32:29": "often used differently than human",
  "32:30": "decision-makers in practice and so this",
  "32:33": "is something where people will often",
  "32:35": "kind of talk about them as though",
  "32:37": "they're plug-and-play",
  "32:38": "interchangeable but in in kind of in use",
  "32:44": "it's not the case people are more likely",
  "32:46": "to assume algorithms are objective or",
  "32:48": "airfry this is also true even if you",
  "32:51": "portray you know I'm just giving this",
  "32:53": "human a recommendation or a guide people",
  "32:56": "often kind of take that as more of an",
  "32:58": "authority algorithms are more likely to",
  "33:02": "be implemented with no appeals process",
  "33:04": "in place and I think this comes because",
  "33:07": "often algorithmic or automated decision",
  "33:09": "systems are being implemented as a",
  "33:11": "cost-cutting measure and allowing",
  "33:14": "appeals or recourse is more expensive",
  "33:18": "and so the motivation when it's around",
  "33:20": "cost-cutting and not around improving",
  "33:22": "accuracy is often not going to include a",
  "33:24": "process for recourse algorithms are more",
  "33:28": "likely to be used at scale so they can",
  "33:29": "be replicating identical biases at scale",
  "33:32": "and algorithmic systems are cheap and I",
  "33:35": "think there's a lot of interplay kind of",
  "33:37": "between between these factors kathy",
  "33:40": "o'neill wrote in weapons of mass",
  "33:42": "destruction that the privileged are",
  "33:44": "processed by people the poor are",
  "33:45": "processed by algorithms",
  "33:47": "alright so kind of in summary humans are",
  "33:51": "biased why does algorithmic bias matter",
  "33:53": "and the reasons for that are machine",
  "33:55": "learning can create feedback loops as we",
  "33:57": "saw before so when it's help creating",
  "33:59": "outcomes can amplify bias it's used",
  "34:02": "differently and then also that I think",
  "34:04": "technology is very powerful and that",
  "34:06": "there's a responsibility with that I'll",
  "34:09": "start a starting next session so now I",
  "34:12": "want to talk about it's not just the",
  "34:14": "data and so this was kind of a focus on",
  "34:16": "okay this is ways bias can appear in the",
  "34:18": "data so something important to",
  "34:21": "understand so there's a joke you know",
  "34:23": "the the good thing about computers is",
  "34:25": "they do exactly what people tell them to",
  "34:26": "the bad thing is they do exactly what",
  "34:28": "people tell them to and often you you",
  "34:31": "know may mean something in your head",
  "34:33": "that",
  "34:34": "not what makes it into what you what you",
  "34:35": "type and in machine learning in",
  "34:39": "particular you know a person is the one",
  "34:42": "that is defining what is success and",
  "34:45": "that typically takes the form of we're",
  "34:47": "minimizing an error function or",
  "34:50": "maximizing some sort of score but who",
  "34:54": "gets defined to define that error",
  "34:56": "function and what the the.21 definitions",
  "34:59": "talk was kind of touching on was also",
  "35:01": "that that's not not necessarily kind of",
  "35:03": "straightforward what what error function",
  "35:05": "you choose and so arvind shared this",
  "35:11": "chart from wikipedia on ways to evaluate",
  "35:14": "binary classifiers and so here kind of",
  "35:19": "the the top left is giving this notion",
  "35:23": "of true positive true negative a false",
  "35:26": "positive and a false negative and then",
  "35:29": "there are all these different ways to",
  "35:31": "combine these into accuracy and negative",
  "35:34": "predictive value and false discovery",
  "35:36": "rate and f1 score and different",
  "35:38": "different scores you can make to",
  "35:40": "evaluate how good is your classifier and",
  "35:42": "so to give an example if you were using",
  "35:45": "if you're doing a medical diagnosis",
  "35:47": "maybe something like a mammogram trying",
  "35:50": "to identify you know is there cancerous",
  "35:52": "tumor here what this is this is kind of",
  "35:57": "a value judgment but how how does a",
  "35:59": "false negative compare to a false",
  "36:01": "positive in this instance yes I would",
  "36:07": "agree with that so false negative is",
  "36:09": "you're telling someone you don't have",
  "36:11": "cancer and they do which is which is bad",
  "36:14": "a a false positive in that case is",
  "36:16": "telling someone hey you may have cancer",
  "36:19": "and they don't and presumably they're",
  "36:20": "gonna go then get a biopsy and find out",
  "36:23": "they don't and there there is still some",
  "36:24": "cost to that you've caused stress for a",
  "36:26": "person this is another procedure they",
  "36:29": "have to have done but yeah I'm on the",
  "36:31": "whole that definitely seems less bad",
  "36:33": "than telling someone they don't have",
  "36:34": "cancer when they do how about a spam so",
  "36:38": "you're identifying if an email a spam or",
  "36:41": "not and if it is you're gonna put it in",
  "36:43": "their spam fold or not even show it to",
  "36:45": "them this is",
  "36:46": "you know your email server what do you",
  "36:50": "think is worse here or false positive or",
  "36:51": "false negative I think so yeah so this",
  "36:57": "is a false positive",
  "36:58": "oh okay and what what was the email if",
  "37:05": "you want to you'll have to say yeah so",
  "37:17": "yeah it's an email from a loved one or",
  "37:19": "about a job offer something important",
  "37:22": "you don't want it going to your",
  "37:23": "promotions tab or your spam folder and",
  "37:26": "with and with all these so you can also",
  "37:27": "think about the trade off so I would I",
  "37:29": "would totally agree on that but there's",
  "37:31": "a point where you know getting one spam",
  "37:33": "email and your regular inbox like hey",
  "37:36": "that's okay but if you get ten spam",
  "37:39": "emails in your regular inbox you know",
  "37:41": "there's probably some tipping point",
  "37:42": "where you're like this is stop cutting",
  "37:44": "the spam phishing emails in my regular",
  "37:45": "inbox and ditto with the cancer",
  "37:50": "screaming screaming example right",
  "37:53": "another another case how about going",
  "37:56": "back to this criminal recidivism",
  "37:57": "algorithm so predicting if a defendant",
  "38:01": "is low risk or high risk for committing",
  "38:03": "another crime",
  "38:06": "when I say committing another crime but",
  "38:08": "this includes people that haven't even",
  "38:09": "had a trial",
  "38:12": "so what's worse false positive or false",
  "38:15": "negative",
  "38:19": "so mostly here false positives you know",
  "38:24": "if you pass this case today again",
  "38:30": "basically for don't get the person to",
  "38:36": "say that basically we can try and say",
  "38:39": "you have to come back to jail because",
  "38:41": "you don't get paid it was too high",
  "38:43": "likely to commit a crime obviously and",
  "38:46": "you would obviously person is the other",
  "38:48": "way around is you're releasing people by",
  "38:50": "submitting crimes then they have",
  "38:52": "actually invited their victims of the",
  "38:53": "crime right so who who you ask there",
  "38:57": "pensive yes yes who you ask will",
  "39:00": "definitely vary your answer I see",
  "39:02": "another hand in the fourth row pass the",
  "39:13": "idea of a system of justice having its",
  "39:14": "own moral Center so if you have a",
  "39:16": "Jeffersonian statement like it's better",
  "39:19": "to let ten guilty people go them and",
  "39:20": "said well put one innocent person in",
  "39:22": "jail and you come up with systems where",
  "39:24": "you can simply undercut that principle",
  "39:27": "because it's profitable for you to",
  "39:29": "imprison more people or so on then the",
  "39:32": "idea there is no moral center to begin",
  "39:33": "with and we have to redefine it I think",
  "39:35": "boils back to no there it is we're just",
  "39:38": "ignoring it and so in that case all",
  "39:41": "positives are clearly bad unless you",
  "39:42": "disagree with the system I mean I think",
  "39:46": "so you're kind of saying that like with",
  "39:48": "a for-profit prison system you're not",
  "39:51": "even trying to hold to an idea of it's",
  "39:54": "better to what ten guilty people go free",
  "39:57": "then put one innocent person in prison I",
  "40:00": "think that I don't know that everyone",
  "40:02": "would agree with that statement though I",
  "40:05": "mean I agree that there's a lot of",
  "40:07": "problems with a for-profit prophet",
  "40:08": "prison system but I think even in a",
  "40:11": "healthier system I think there are",
  "40:15": "people that would say like law and order",
  "40:17": "is a high ideal and might not agree with",
  "40:19": "the letting ten guilty people go free so",
  "40:24": "I think there is still something of a",
  "40:25": "value judgment there although I do think",
  "40:28": "it's definitely clouded by kind of other",
  "40:29": "factors is that they're so fast specific",
  "40:40": "what was the crime is it a misdemeanor",
  "40:42": "versus like in yeah everything is so",
  "40:45": "fact specific as to the the false",
  "40:48": "negative and then the criminal goes and",
  "40:50": "is a recidivist and murders another",
  "40:51": "person yeah it's very difficult to just",
  "40:54": "container these and in general all of",
  "40:56": "these so much spam you know ya know",
  "40:59": "there is a lot of yeah nuance that is",
  "41:01": "not going to necessarily be captured",
  "41:04": "yeah so this was to kind of to highlight",
  "41:07": "that there's not",
  "41:09": "that your perspective does matter and",
  "41:11": "that I mean what when some of these we",
  "41:14": "had more agreement then then others",
  "41:19": "aren't you know some of these I think",
  "41:20": "it's possible to reach broader consensus",
  "41:22": "than others that there's not necessarily",
  "41:24": "a clear-cut answer and then even more so",
  "41:27": "there's not necessarily a clear cut this",
  "41:28": "is the best way to evaluate the the",
  "41:31": "error rate for for a given scenario all",
  "41:37": "right then it's about a seven o'clock so",
  "41:39": "let's stop here for our seven minute",
  "41:43": "break and then we'll resume back let's",
  "41:46": "start back up I want to check if there",
  "41:49": "are any questions about the twenty one",
  "41:50": "definitions video or any thoughts that I",
  "41:55": "wanted to move on to the problem",
  "41:57": "formulation and fairness paper and so",
  "42:01": "this was a ethnographic field study",
  "42:04": "where they followed this team of kind of",
  "42:06": "data scientist and product managers",
  "42:08": "business analyst product managers and so",
  "42:13": "on a company that was working with car",
  "42:17": "companies to pass on leads to kind of",
  "42:22": "two otter auto dealers and the initial",
  "42:25": "project goal was to improve the quality",
  "42:27": "of the leads actually curious what did",
  "42:29": "what do people think of this paper",
  "42:31": "there's any thoughts or reactions oh ok",
  "42:35": "so I'll repeat it for the microphones",
  "42:36": "one said they were mad they care so much",
  "42:38": "about their score any other thoughts hey",
  "42:46": "and I'll say like I thought it was kind",
  "42:49": "of interesting to see having worked as a",
  "42:52": "data scientist someone described the",
  "42:53": "process of what to me felt very familiar",
  "42:56": "in this kind of academic and studied way",
  "42:59": "but I thought that was helpful and it",
  "43:02": "started out with this question of what",
  "43:04": "makes a lead high-quality and there were",
  "43:08": "various answers from various people in",
  "43:09": "the project you know it could be the",
  "43:12": "buyer salary whether the car the car",
  "43:14": "they want is in stock which is neat kind",
  "43:16": "of thinking about from the buyers",
  "43:19": "perspective what they're looking for",
  "43:22": "dealer specific finance abilities so",
  "43:25": "different dealers have different",
  "43:26": "financing processes and so they kind of",
  "43:31": "you know started out with this broad",
  "43:32": "brainstorm of like oh these are the",
  "43:34": "things that could make a lead",
  "43:36": "high-quality and then ran into",
  "43:38": "difficulties the dealers won't share",
  "43:40": "that data the credit scores were",
  "43:43": "segmented into different ranges so they",
  "43:45": "kind of had these coarse ranges that",
  "43:46": "didn't really align and so they end up",
  "43:50": "reducing the problem to predicting the",
  "43:51": "credit score below 500 versus above 500",
  "43:54": "and so it's kind of an disappointing but",
  "43:57": "it's a trajectory I have been on where",
  "43:59": "they I feel like start with this you",
  "44:00": "know broader and more interesting",
  "44:02": "question about high quality leads and",
  "44:05": "then they have to boil it down to well",
  "44:07": "we kind of don't really have any data",
  "44:08": "other than these big buckets for credit",
  "44:11": "score so let's go with that they did",
  "44:15": "talk about you know should they try to",
  "44:17": "purchase higher quality data sets but",
  "44:19": "ruled that those were too expensive and",
  "44:22": "the project ended up failing and so it",
  "44:25": "was I thought kind of helpful to go on",
  "44:27": "this trajectory and it's one that I have",
  "44:28": "witnessed firsthand that yeah that often",
  "44:33": "I think projects are kind of limited by",
  "44:36": "the the data that you have and I think",
  "44:38": "that's important to keep in mind and",
  "44:40": "keep in mind also these like business",
  "44:42": "constraints that that really matter in",
  "44:44": "the workplace when discussing these",
  "44:47": "problems because I think it can be easy",
  "44:48": "to kind of focus on you know a more",
  "44:52": "academic or idealize like oh it'd be",
  "44:54": "great if we had you know this type of",
  "44:56": "data or could do this sort of process",
  "44:58": "but that may often not work out",
  "45:03": "different people gave different reasons",
  "45:06": "that the project failed whether it was",
  "45:08": "the data quality the expectations the",
  "45:10": "the nature of what they are trying to do",
  "45:14": "so something I want to ask you is what",
  "45:16": "and for this week I want you to try",
  "45:19": "talking in groups of three or four again",
  "45:21": "and so yeah you can pair up with people",
  "45:24": "near you or you can also cross rows and",
  "45:28": "we'll just take maybe seven minutes to",
  "45:30": "discuss just if what if anything do you",
  "45:33": "think this team could have done",
  "45:34": "differently to consider bias and fair",
  "45:36": "I mean do you think they should have",
  "45:37": "done it anything differently and if so",
  "45:40": "what and if not why not",
  "45:42": "kind of curious to hear well does anyone",
  "45:45": "wanna does anyone want to share",
  "45:47": "something that their group discussed the",
  "45:51": "catch box is sitting on the third row I",
  "45:53": "don't know if anyone on the third row",
  "45:54": "wants to say anything or pass it forward",
  "45:56": "to the second yeah so we were just",
  "46:00": "discussing that it's kind of a question",
  "46:02": "of alignment where when you're saying",
  "46:03": "you're looking for quality that can kind",
  "46:06": "of you can have these fuzzy beliefs",
  "46:07": "about what quantity man and that be but",
  "46:09": "when you don't kind of other",
  "46:10": "company-wide alignment when everyone",
  "46:12": "agrees what it means that when you vote",
  "46:14": "I'm telling you what a machine what",
  "46:16": "quality means that's kind of when it",
  "46:18": "gets reductive and bias almost a sort of",
  "46:20": "the mission so you can't really",
  "46:22": "eliminate the bias cuz they're trying is",
  "46:24": "how can we figure out the credit score's",
  "46:25": "for cheaper and there's this quote from",
  "46:28": "ma che sick mouse key that machine",
  "46:30": "learning is just money laundering for",
  "46:32": "bias that bias laundering because",
  "46:35": "quality wasn't defined what it ended up",
  "46:37": "being is just saying well let's try to",
  "46:39": "do bias but with this machine so nobody",
  "46:42": "knows so we kind of had the pessimistic",
  "46:45": "view that you couldn't really eliminate",
  "46:47": "bias from it because unless quality sort",
  "46:50": "of had a rigorous definition before they",
  "46:52": "started this project then bias is kind",
  "46:54": "of what they were trying to achieve oh",
  "46:55": "one thing that came to mind for me was",
  "46:58": "just changing the business model to",
  "47:02": "accommodate the idea of selling more",
  "47:03": "cars more people buy the modern maid",
  "47:06": "leasing model we have today is some",
  "47:08": "perversion of the initial model but it",
  "47:10": "was all aimed at depreciating the value",
  "47:12": "of the car and looking at the secondary",
  "47:14": "market before you sold that car and the",
  "47:17": "end result was you could go to any",
  "47:19": "particular customer say do you have X",
  "47:21": "amount down down payment and can you",
  "47:24": "afford $380 a month that they could",
  "47:26": "answer that knowing full well that the",
  "47:28": "consequences or lying were immediate",
  "47:29": "repossession they automatically",
  "47:31": "qualified themselves out from the stress",
  "47:32": "of losing a car that they just got so",
  "47:36": "just changing the model to address your",
  "47:38": "market rather than trying to figure out",
  "47:40": "you you know your customer your",
  "47:42": "potential customer by data you can't",
  "47:43": "have or don't want to pay for see like",
  "47:45": "one way to go about that yeah and that's",
  "47:47": "good point also about just when you have",
  "47:49": "clear simple rules as opposed to you",
  "47:52": "know even the credit score itself is",
  "47:54": "kind of this opaque system but you're",
  "47:57": "suggesting something with clear rules of",
  "48:00": "if you have this much money and can pay",
  "48:02": "this much money per month then you can",
  "48:04": "get actor well thank you all that was",
  "48:07": "interesting I didn't have like a I was",
  "48:09": "actually not sure what you would come up",
  "48:10": "with but I appreciated those answers and",
  "48:12": "and thinking about it and and some of",
  "48:15": "this stuff um check us in the privacy",
  "48:17": "and surveillance lesson we'll talk a",
  "48:19": "little bit more about financing both the",
  "48:24": "big three in the US as well as some kind",
  "48:26": "of micro financing projects as well",
  "48:33": "alright so hopefully so I kind of in the",
  "48:37": "the first part you know wanted to make a",
  "48:39": "case that algorithmic bias matters and",
  "48:42": "is significant I also want to highlight",
  "48:44": "that bias isn't the whole story or the",
  "48:46": "only issue and some of you have already",
  "48:48": "kind of touched on this and you're kind",
  "48:50": "of in your comments first that I think",
  "48:53": "accuracy is not it's not everything",
  "48:56": "and so referring back to the compass",
  "49:00": "recidivism algorithm point that Abe Gong",
  "49:03": "shared he's a data scientist part of the",
  "49:09": "input to the system is this",
  "49:11": "questionnaire that the defendants fill",
  "49:14": "out and it includes questions such as if",
  "49:17": "you lived with both parents and they",
  "49:18": "later separated how old were you at the",
  "49:20": "time was your father figure or father",
  "49:23": "figure who principally raised you ever",
  "49:25": "arrested that you know of and so keep in",
  "49:27": "mind these questions are then inputs to",
  "49:31": "this algorithm that's helping determine",
  "49:33": "who who has to pay parole or sorry pay",
  "49:37": "bail who might get parole really",
  "49:40": "significantly impacting someone's life",
  "49:42": "and as we saw from the kind of Dartmouth",
  "49:44": "study i en't mentioned this is actually",
  "49:47": "not very accurate and there's not",
  "49:48": "evidence that this makes it more",
  "49:49": "accurate but even if it did I think this",
  "49:53": "is unethical to have these sorts of",
  "49:56": "factors that you used in making such a",
  "49:59": "decision just in that",
  "50:01": "you know people have no control over",
  "50:02": "what happened to them as children or",
  "50:05": "what their parents may have done and it",
  "50:06": "seems me kind of very unethical to use",
  "50:08": "that in someone's prison sentence and so",
  "50:11": "I think it's important to keep in mind",
  "50:13": "that kind of improving accuracy",
  "50:15": "shouldn't be be your only goal but that",
  "50:18": "there may still be types of data or",
  "50:20": "particular factors that are unethical to",
  "50:22": "use I remember reading this and it",
  "50:31": "didn't go into it said well yeah this is",
  "50:45": "the other the other factor is that it",
  "50:47": "said and I actually don't know the",
  "50:50": "answer to this but my understanding is",
  "50:51": "that people could lie on this",
  "50:53": "questionnaire and so then are you yeah",
  "50:55": "rewarding people that lie or write so",
  "51:05": "that yeah so there is a probably",
  "51:08": "significant significant yeah like who",
  "51:10": "knows what people were saying for this I",
  "51:13": "mean I can imagine some people may have",
  "51:15": "felt intimidated and felt like they",
  "51:17": "needed to give correct answers well it's",
  "51:19": "140 inputs they're not all questions and",
  "51:23": "I actually don't know yeah what what",
  "51:27": "else is in the question yeah so this you",
  "51:28": "should raise a lot of issues like yeah",
  "51:30": "this just seems like terrible for many",
  "51:32": "reasons",
  "51:38": "all right so yeah that's one kind of",
  "51:41": "just issue apart from bias but that is",
  "51:44": "relevant yeah then kind of returning to",
  "51:50": "to facial recognition and it's important",
  "51:55": "to think about its uses so it's kind of",
  "51:57": "wrong to have these very very different",
  "51:59": "error rates the same time in in 2015",
  "52:04": "after Freddie gray a black man was",
  "52:07": "killed by police in Baltimore Baltimore",
  "52:09": "police used facial recognition to",
  "52:12": "identify protesters protesting his death",
  "52:15": "and this was specifically to identify",
  "52:20": "people that had existing warrants",
  "52:21": "although there's no information on what",
  "52:23": "these warrants were for if they were for",
  "52:25": "very minor offences or not and this was",
  "52:30": "from this private company and this only",
  "52:32": "came to light because the ACLU ended up",
  "52:36": "obtaining some of its marketing",
  "52:37": "materials and the company was bragging",
  "52:39": "about this is like a successful case",
  "52:41": "study they even I should have included",
  "52:43": "the language but it's like oh it's so",
  "52:45": "great we were able to like arrest these",
  "52:47": "people before they harmed anyone but it",
  "52:49": "was like why why were you arresting them",
  "52:51": "if they if they hadn't done anything and",
  "52:54": "so this is this is really concerning I",
  "52:56": "think the idea of identifying protesters",
  "53:00": "you know if this became widespread or",
  "53:03": "even not widespread if this became you",
  "53:07": "know was being used more I think could",
  "53:09": "really impact civil rights and so it's",
  "53:12": "important to think about applications",
  "53:14": "like this of this is not something we're",
  "53:16": "you know having less biased facial",
  "53:19": "recognition I mean this is you know it'd",
  "53:21": "be bad to be making errors here but it's",
  "53:23": "also bad to be doing this accurately",
  "53:24": "like this is not I think this is a not a",
  "53:27": "good use case at all and so wanted to",
  "53:30": "wanted to highlight that",
  "53:33": "yes a question in particular yeah the",
  "53:41": "police is doing it's like at the end is",
  "53:45": "like they are finding ways to do what",
  "53:48": "they need to do in a cheaper way right",
  "53:50": "and that's I think what pushed them to",
  "53:51": "actually implement this system right",
  "53:54": "because so in general I would say",
  "53:58": "because even for example I don't know",
  "54:01": "all the companies in the US who decide",
  "54:03": "okay yeah no we are not going to develop",
  "54:05": "these technologies because we know all",
  "54:07": "this stuff is going on and there's",
  "54:09": "dieter 360 things and so we are not",
  "54:11": "going to develop technologies right but",
  "54:13": "then I can't break the need or the",
  "54:15": "pressure pattern of the police or the",
  "54:18": "type of organization to provide these",
  "54:19": "technologies because my sister like that",
  "54:22": "I don't know maybe they will start",
  "54:24": "buying can these technologies why are",
  "54:27": "they develop we are there's no this",
  "54:29": "person right so so the question is like",
  "54:32": "there's some Drive so one is how you",
  "54:36": "approach and develop a technology but",
  "54:37": "somehow there's so drive or for",
  "54:41": "forgetting these technologies that are",
  "54:42": "not so clear for me what is the source",
  "54:44": "and so how you need to control that for",
  "54:46": "the people decided to buy these",
  "54:47": "technologies so they don't so I would I",
  "54:50": "would disagree with your original",
  "54:52": "statement that the police are a that",
  "54:55": "they need to do this and that be this is",
  "54:57": "allowing them to do it cheaper there are",
  "54:59": "many things that are very inefficient",
  "55:02": "about the market for technology sold to",
  "55:06": "police so in the and we'll talk more",
  "55:08": "about this in the privacy and",
  "55:09": "surveillance lesson but just briefly so",
  "55:12": "one policing in the u.s. is kind of",
  "55:15": "hyper local so these decisions are kind",
  "55:17": "of made department by Department but",
  "55:19": "often I don't know that tech companies",
  "55:22": "are identifying like hey this is a",
  "55:25": "legitimate and necessary need that were",
  "55:28": "fulfilling they're often kind of pushing",
  "55:30": "like hey isn't this technology cool and",
  "55:33": "selling it there and it can be very",
  "55:36": "expensive in many cases so for many",
  "55:40": "types of technology there's kind of near",
  "55:42": "monopoly so for instance police body",
  "55:43": "cameras",
  "55:44": "taser now",
  "55:46": "axon has virtual monopoly on the market",
  "55:50": "I sell so I'll post an article I wrote",
  "55:52": "with kind of some relevant pieces of",
  "55:54": "information but yeah I would disagree",
  "55:56": "that this was something that needed to",
  "55:58": "happen or that would be done either way",
  "56:00": "and also even that this is lowering the",
  "56:02": "cost because these can be quite",
  "56:04": "expensive and many of these companies",
  "56:05": "are then kind of pushing cloud storage",
  "56:08": "of keeping this data forever but this is",
  "56:11": "I would prefer to wait to the kind of",
  "56:16": "privacy and surveillance and mostly I'm",
  "56:20": "kind of highlighting this to show that",
  "56:21": "even in you know and as we discussed",
  "56:25": "earlier you are probably never going to",
  "56:28": "get the exact same accuracy on different",
  "56:30": "measures across groups but even if you",
  "56:32": "had significantly less biased",
  "56:34": "facial-recognition I think their",
  "56:36": "applications of it that would be very",
  "56:37": "questionable and so it's not just a",
  "56:39": "question of removing bias but also",
  "56:41": "thinking of kind of how how the",
  "56:42": "technology is being used and sorry those",
  "56:46": "I realize more editorializing here if",
  "56:49": "you but yeah so that is my kind of",
  "56:55": "opinion that it is very would be very",
  "56:57": "harmful to be identifying protesters and",
  "57:05": "that this is something that's happened",
  "57:06": "in the United States there was the",
  "57:11": "digital of justice lab hosted a workshop",
  "57:13": "unfortunately in Boston called please",
  "57:16": "don't include us of groups kind of not",
  "57:19": "wanting to be included in kind of",
  "57:22": "creating a more or you know less biased",
  "57:25": "technology because of concerns about how",
  "57:27": "it was being used and so kind of many",
  "57:30": "others have kind of raised these these",
  "57:32": "concerns and this happened this fall and",
  "57:36": "I was not able to find much much",
  "57:37": "information about it but I was very or",
  "57:39": "something where I was like if this was",
  "57:40": "local I would have I would have loved to",
  "57:41": "go another kind of issue and this gets",
  "57:48": "back a little bit to what Aly was saying",
  "57:49": "earlier about kind of race and gender",
  "57:52": "being social constructs these are two",
  "57:54": "papers by trans researchers on the",
  "57:57": "problem with",
  "57:59": "doing gender for gender classification",
  "58:00": "using facial recognition and so so first",
  "58:05": "of all kind of doing gender",
  "58:07": "classification with facial recognition",
  "58:08": "has much higher error rates for for",
  "58:11": "trans and non-binary people but it's not",
  "58:15": "I mean so that that's an example of bias",
  "58:18": "you have you know higher error rates on",
  "58:19": "certain groups however particularly in",
  "58:22": "the AH skis paper they argue that the",
  "58:27": "goal shouldn't be like oh let's just you",
  "58:29": "know improve the air rates on trans and",
  "58:31": "non-binary people it's that the whole",
  "58:33": "premise of you know that gender is",
  "58:35": "something you're assigning to someone",
  "58:38": "externally you know whether that's by a",
  "58:41": "person kind of looking at you or a",
  "58:44": "computer you know taking your image and",
  "58:46": "then determining this for you but that",
  "58:48": "that whole kind of premises is incorrect",
  "58:50": "and not something to be pursuing and so",
  "58:53": "that's kind of another example where",
  "58:54": "it's not just oh let's gather a more",
  "58:57": "diverse data set and then we can can",
  "58:59": "reduce our bias but that the the premise",
  "59:01": "may be incorrect and I joined Balan we",
  "59:07": "need has spoken about this that that",
  "59:09": "algorithmic fairness is not justice and",
  "59:11": "they're kind of many many issues that",
  "59:13": "aren't captured questions kind of on",
  "59:20": "this part of yeah like that they're",
  "59:23": "places that we just shouldn't be using",
  "59:24": "facial recognition and it's not just",
  "59:26": "okay let's get police more accurately",
  "59:29": "identifying black black women but let's",
  "59:32": "really think about how it's being used",
  "59:34": "Ali published paper uh kinda cousins",
  "59:43": "recently that I had the pleasure of",
  "59:46": "presenting critiquing the idea of",
  "59:51": "fairness and even accountability",
  "59:52": "transparency where they were suggesting",
  "59:55": "like let's turn old people into food and",
  "59:56": "let's do it fairly and let's do it",
  "59:58": "accountability",
  "60:01": "they they like provably argue that you",
  "60:05": "can do all of these things fair ways but",
  "60:07": "like if the point of it at the end is to",
  "60:08": "turn people into food then this is like",
  "60:11": "completely misguided in deference yeah",
  "60:17": "yeah but so I like Joyce quite correctly",
  "60:23": "like that was a similar sort of like",
  "60:25": "ethos that she was going for which is",
  "60:27": "that like you can have like fairness of",
  "60:29": "the application of the algorithmic",
  "60:30": "system but if the algorithmic system is",
  "60:32": "there to hurt all of us or to across all",
  "60:34": "of us like it's fair it's oppression I",
  "60:37": "guess but that's awful yeah Thank You",
  "60:41": "Ellie and I'll post a link to the the",
  "60:43": "mulching proposal paper cuz I read that",
  "60:45": "and that is that is a great example hey",
  "60:47": "at the end of end of your row because if",
  "61:15": "you can say to data business sucks Co",
  "61:21": "this guy might be I mean we going to be",
  "61:29": "like if that's the case and I think",
  "61:51": "you're getting a many of kind of",
  "61:54": "concerns around surveillance of",
  "61:56": "collecting a lot of data yeah if we",
  "61:59": "don't have safeguards in place that it",
  "62:01": "could be used in pretty bad ways and",
  "62:04": "yeah we will talk more about that but",
  "62:07": "yeah I think there are definitely",
  "62:08": "concerns of kind of if you you know and",
  "62:12": "a lot of this even just goes into",
  "62:13": "insurance but it's like if we get super",
  "62:15": "accurate and they're correct collecting",
  "62:16": "tons of data yeah like health insurance",
  "62:18": "could become even more inaccessible for",
  "62:22": "people that are predicted to to be on",
  "62:25": "healthier to have issues so so yeah well",
  "62:28": "we'll talk about some of those issues",
  "62:29": "kind of in the privacy week not me we",
  "62:39": "move on but I'll take some more",
  "62:40": "questions kind of in a moment and",
  "62:43": "another paper I thought was really",
  "62:45": "interesting this is from my CML last",
  "62:48": "year and was on a fare washing and here",
  "62:52": "they showed that if you had a unfair",
  "62:55": "algorithm basically you could go back",
  "62:58": "given a fairness definition and make a",
  "63:00": "kind of post-talk justification to make",
  "63:03": "it seem fairer than it was but still",
  "63:05": "come up to the same answers and so this",
  "63:08": "is kind of like from a mathematical",
  "63:09": "perspective but I thought was really",
  "63:11": "interesting and so this also kind of",
  "63:14": "shows that you know meaning some",
  "63:16": "fairness criteria will not be sufficient",
  "63:18": "as you could still be kind of hiding the",
  "63:20": "the real reasons your making decisions",
  "63:28": "okay so then in the kind of the the",
  "63:31": "final section I want to talk about some",
  "63:33": "steps towards solutions and as you as",
  "63:36": "you may have guessed there's no kind of",
  "63:38": "oh the solves it and and now you're done",
  "63:40": "but some of the I think kind of positive",
  "63:43": "steps towards solutions and then also",
  "63:45": "want to note that next week we'll we'll",
  "63:48": "be talking more about kind of processes",
  "63:49": "to implement and thinks that could be",
  "63:51": "helpful kind of practically in in terms",
  "63:54": "of working towards solutions and the",
  "63:56": "first and this is kind of adopted from a",
  "63:59": "talk I gave but I really encourage",
  "64:01": "people to kind of go back and analyze a",
  "64:04": "project at your workplace or in your",
  "64:07": "school as a kind of concrete thing you",
  "64:09": "can do to kind of look for kind of look",
  "64:13": "for types of bias also starting with the",
  "64:17": "the question of should we even be doing",
  "64:20": "this and remembering",
  "64:22": "I think bias in particular because it's",
  "64:25": "getting more attention",
  "64:26": "on the whole is positive and really glad",
  "64:28": "that it's been covered in the news more",
  "64:30": "often leads people to potentially think",
  "64:34": "though that like any project can be d",
  "64:35": "biased and then it's it's good and so a",
  "64:39": "paper that will be will be reading in a",
  "64:43": "later week is when the implication is",
  "64:45": "not to design and this kind of says you",
  "64:49": "know engineers tend to respond to",
  "64:50": "problems with you know like what can I",
  "64:52": "make or build to fix this and I think",
  "64:54": "that can be coming from a really good",
  "64:55": "place of you know like these are the",
  "64:57": "tools I have what can i what can I do",
  "64:59": "but sometimes the answer is to not to",
  "65:01": "not make her build anything and so some",
  "65:04": "examples I think of really troubling",
  "65:06": "technology include the facial",
  "65:10": "recognition for ethnicity recognition",
  "65:14": "and said these were papers that were",
  "65:16": "distinguishing between Chinese we Gers",
  "65:18": "Tibetans and Koreans and the Chinese we",
  "65:21": "are the Muslim minority in western China",
  "65:24": "that are being put in internment camps",
  "65:27": "there's also been at least two research",
  "65:29": "product projects now about trying to",
  "65:32": "identify people's sexuality from their",
  "65:34": "pictures and it's you know it's really",
  "65:39": "what they're doing is identifying kind",
  "65:41": "of cultural differences and in kind of",
  "65:44": "how people are presenting themselves on",
  "65:45": "dating sites I believe for the most part",
  "65:48": "but just that this whole idea is kind of",
  "65:50": "a problem of you know a this could be",
  "65:52": "very dangerous for people in many parts",
  "65:54": "of the world if it was I mean again",
  "65:57": "these are these things are often like",
  "65:58": "they're bad if they're wrong and they're",
  "66:00": "also bad if they if they were to work",
  "66:02": "accurately as well as just the premise",
  "66:05": "that sexuality is something you assign",
  "66:07": "to somebody by by looking at them so",
  "66:11": "those are some kind of examples of",
  "66:12": "things to to not do they're not we'll",
  "66:15": "talk more about those later and so",
  "66:19": "really kind of starting there but from",
  "66:20": "there if you've determined it's you know",
  "66:22": "a project that that is good to be doing",
  "66:24": "looking for what biases and the data and",
  "66:27": "recognizing that all data is biased if",
  "66:30": "there's not kind of unbiased and I think",
  "66:33": "someone said this earlier there's not",
  "66:34": "unbiased data out there but the",
  "66:36": "important thing is to kind of understand",
  "66:38": "it and understand how is",
  "66:39": "they're also looking for kind of what",
  "66:43": "accountability mechanisms are in place",
  "66:45": "you know can the the code and data here",
  "66:48": "be audited what our error rates for",
  "66:51": "different subgroups what is the accuracy",
  "66:54": "of a simple rule based alternative and",
  "66:56": "this is something that I mean this is a",
  "66:59": "good first step kind of friend a machine",
  "67:01": "learning project is just to know what",
  "67:03": "can you get with a simple rule based",
  "67:06": "alternative and you know something we",
  "67:07": "saw with the compass recidivism",
  "67:08": "algorithm where it's not even more more",
  "67:11": "accurate than a linear classifier on",
  "67:13": "three variables but to kind of know you",
  "67:16": "know is my algorithm even that accurate",
  "67:18": "or doing what I think it is question in",
  "67:20": "the fourth row are you aware of any",
  "67:27": "studies comparing like a machine",
  "67:29": "learning based approach against a rules",
  "67:31": "engine and just saying which of these is",
  "67:32": "you know do we have any provable",
  "67:34": "evidence that one can work better than",
  "67:36": "the other for any given data set or any",
  "67:39": "given question",
  "67:41": "Elise Chi yeah so I'm not sure if",
  "67:49": "there's evidence that rule-based systems",
  "67:51": "perform better but there is some a",
  "67:55": "substantial amount of evidence a lot of",
  "67:57": "it coming from Scirocco well who's at",
  "67:59": "Stanford in Management Sciences",
  "68:01": "basically showing that rule based",
  "68:04": "systems like really simple rule based",
  "68:05": "systems where like a judge would like",
  "68:07": "have a scorecard and it would basically",
  "68:08": "say like has this person committed a",
  "68:09": "crime it's like an previously to this",
  "68:12": "bail hearing if so at four points",
  "68:14": "something at the end of that like if",
  "68:15": "it's only this threshold and you know",
  "68:18": "don't get me mail that that performs",
  "68:20": "like so close to as well as the top of",
  "68:24": "the line AI systems that were around at",
  "68:26": "the time that it it was extremely",
  "68:28": "difficult for them to provide any",
  "68:30": "motivated reason for why you'd use a",
  "68:33": "very complex machine learning system I'm",
  "68:37": "not sure whether it's demonstrably",
  "68:39": "possible to say they're like a",
  "68:40": "rule-based system were actually",
  "68:41": "outperformed guys but because they I",
  "68:44": "would just sort of start to kind of",
  "68:46": "consume that but yeah there there's part",
  "68:49": "of it that shows that it's so close and",
  "68:51": "they deleted it",
  "68:53": "in why you came to that conclusion is",
  "68:57": "the notes you kind of from more of a",
  "68:59": "philosophical point I think it can",
  "69:01": "provide a kind of consistency that seems",
  "69:04": "very fair to say I kind of know why I'm",
  "69:06": "doing it for rules that I think are fair",
  "69:08": "and are being applied consistently then",
  "69:16": "as I mentioned before kind of having a",
  "69:18": "place to away to handle appeals and",
  "69:22": "mistakes and recognizing that computers",
  "69:25": "make mistakes data has airs can you",
  "69:28": "catch them before kind of something",
  "69:30": "disastrous happens then can you kind of",
  "69:33": "write things that are that are wrong and",
  "69:36": "then also looking at the the diversity",
  "69:39": "of the team that built it because I",
  "69:41": "believe that's a factor in in kind of",
  "69:43": "creating creating better technology hand",
  "69:46": "in the fourth row historically in",
  "69:55": "technology it's been a lot of primary",
  "69:58": "white males and so seeing something like",
  "70:01": "the compass where it's like down",
  "70:04": "performing african-americans is that",
  "70:08": "maybe because of the inherent bias of",
  "70:10": "the in-group bias and the people that",
  "70:12": "created it yeah I think that's",
  "70:14": "definitely a factor in general and not",
  "70:17": "just I think it also shows up in kind of",
  "70:22": "not always thinking about what can go",
  "70:24": "wrong and how this technology could be",
  "70:26": "applied but I think that the the",
  "70:30": "homogeneity in tack is has played a role",
  "70:33": "and kind of many of the misuses and",
  "70:37": "negative impacts and then another",
  "70:40": "problem that we may get to get to more",
  "70:43": "laters you know often they're tight",
  "70:45": "NDA's and this is an issue and police",
  "70:47": "use of technology as well often there",
  "70:48": "are these very restrictive NDA's and so",
  "70:50": "you don't even know kind of that this",
  "70:52": "technology exists much less how it's",
  "70:55": "being being applied to you and so I mean",
  "71:01": "I guess one here I was kind of trying to",
  "71:03": "get at the responsibility of those kind",
  "71:06": "of",
  "71:07": "working at a company or help making the",
  "71:09": "tech to recognize the the need for for",
  "71:12": "Appeals I'm I guess as an outsider I",
  "71:15": "think this also highlights in some cases",
  "71:18": "the the need for you know do we need",
  "71:21": "policies to restrict or address the the",
  "71:26": "use of some questionable technology",
  "71:29": "although yeah often just even finding",
  "71:32": "out that it exists can be can be really",
  "71:34": "helpful to even start figuring out",
  "71:37": "what's going on so then a paper that I",
  "71:40": "love is data sheets for data sets this",
  "71:43": "is one of the ones that was in the",
  "71:44": "signed reading by Tim Nick Abreu at all",
  "71:47": "and here Tim it's kind of drawing on her",
  "71:50": "past as an electrical engineer and",
  "71:52": "looking at the kind of electronics",
  "71:57": "industry of you know like circuits and",
  "71:59": "resistors and all these electronics",
  "72:01": "pieces and it's very standardized there",
  "72:04": "to have these data sheets that tell you",
  "72:06": "information about how and when and where",
  "72:09": "they were manufactured and under what",
  "72:11": "conditions are they safe to use and so",
  "72:15": "on and this is kind of a totally totally",
  "72:17": "standard process and so this group",
  "72:20": "proposed something similar for datasets",
  "72:22": "of just recording this information and",
  "72:25": "so this is in keeping with this idea",
  "72:26": "that we can't remove bias from datasets",
  "72:29": "but it would be helpful just to",
  "72:30": "understand how the data set was created",
  "72:33": "and so here they listed kind of some",
  "72:35": "sample questions that could be could be",
  "72:38": "potentially included you know what are",
  "72:41": "the instances in this data who wasn't",
  "72:43": "who was involved in the data collection",
  "72:45": "process over what time frame will the",
  "72:50": "data set be updated this gets to an",
  "72:52": "earlier question about the the recency",
  "72:54": "or if it's changed who was supporting",
  "72:57": "hosting maintaining the data set if the",
  "72:59": "data set relates to people or was",
  "73:01": "generated by people were they informed",
  "73:03": "were they told what the data set would",
  "73:05": "be used for and that they can consent",
  "73:07": "and so on is I think these are kind of",
  "73:09": "really interesting and important",
  "73:11": "questions and I like this idea of",
  "73:12": "standardizing what information you have",
  "73:15": "about a data set because a lot of times",
  "73:17": "bias shows up and I think",
  "73:19": "let's talk about this earlier it's not",
  "73:20": "it's not malicious but it's just because",
  "73:23": "people aren't necessarily thinking and",
  "73:25": "there's something that they've missed",
  "73:26": "and that this is kind of one of the risk",
  "73:28": "of homogeneous groups of if everyone has",
  "73:30": "the same background they're just kind of",
  "73:31": "more likely to all have certain areas",
  "73:34": "that they haven't thought about or kind",
  "73:35": "of miss miss particular worries and dig",
  "73:40": "through data sites that's kind of part",
  "73:41": "of there have been many kind of similar",
  "73:44": "or related proposals including one group",
  "73:47": "framed them as nutrition facts and so",
  "73:50": "similar to kind of the nutrition label",
  "73:52": "that you have around food and how that's",
  "73:54": "that standardized that's something we",
  "73:56": "did not always have but you know",
  "73:58": "developed developed kind of as a safety",
  "74:00": "mechanism and part and a transparency",
  "74:02": "mechanism there was an NLP specific",
  "74:05": "version from Emily bender and bhatia",
  "74:07": "Friedman on data statements for NLP and",
  "74:11": "then there's the standardization of data",
  "74:14": "like licenses the Montreal Data license",
  "74:17": "and so that this is kind of a positive",
  "74:20": "sign when you have researchers in a",
  "74:22": "field coming up with related proposals",
  "74:25": "that kind of many experts are seeing",
  "74:27": "this is a as a promising direction to go",
  "74:29": "in and then so we just read this in the",
  "74:33": "last week to MIT together with you and",
  "74:36": "CEO Joe wrote a new paper that just came",
  "74:40": "out on kind of what could machine",
  "74:42": "learning learn about data collection",
  "74:44": "from archives and library sciences and",
  "74:48": "so in the the field of archives and",
  "74:51": "library sciences a lot more kind of",
  "74:55": "thought and consideration has gone into",
  "74:58": "how datasets are collected and",
  "75:00": "constructed whereas in you know data",
  "75:04": "collection in ml is often kind of pretty",
  "75:06": "haphazard and so this was this was an",
  "75:09": "interesting paper and I'll post a link",
  "75:10": "on just what kind of what lessons can we",
  "75:12": "learn about starting to standardize this",
  "75:15": "and seeing this as an area that requires",
  "75:17": "expertise and thoughtfulness and the",
  "75:21": "development of process",
  "75:24": "certainly just check okay two minutes",
  "75:28": "nice I'll pause though and take because",
  "75:30": "we can that we can finish up next time",
  "75:31": "on this there are questions about about",
  "75:33": "these ideas how I really think that",
  "75:42": "approach is really interesting and",
  "75:43": "always something I think it's great but",
  "75:46": "how hardware projects in particularly",
  "75:49": "have such a different cadence and",
  "75:50": "stopped abruptly and so they allow",
  "75:52": "themselves for that type of",
  "75:54": "documentation in it necessitates a",
  "75:56": "little bit more else it is manufacturing",
  "75:58": "all the things are getting into it and",
  "76:00": "just how you can apply this to so much",
  "76:03": "of what we're talking about it just",
  "76:05": "slowing down Katie's the software",
  "76:06": "projects yeah yes yes yeah that's that's",
  "76:14": "a good point yeah no I heard a talk once",
  "76:16": "and this was it was in a closed",
  "76:18": "environment so I won't quote the person",
  "76:20": "but they won't name the company they",
  "76:22": "were talking about one of the kind of",
  "76:23": "ethics practices that their company was",
  "76:25": "building in pauses and it was something",
  "76:28": "that like sounded like revolutionary",
  "76:29": "when I heard it it was like wow like a",
  "76:30": "tech company is pausing to reflect on",
  "76:34": "what they're doing but yeah I think that",
  "76:35": "that can be that can be really helpful",
  "76:37": "alright then kind of referring to",
  "76:40": "earlier this idea of checking on the",
  "76:42": "accuracy on subgroups it's also",
  "76:45": "important to note that gender Shades was",
  "76:47": "very kind of very deliberately designed",
  "76:50": "to be an experiment that would has a",
  "76:53": "kind of this really tangible practical",
  "76:56": "real-world impact and it's not just",
  "76:58": "chance that it has but that it was very",
  "77:01": "kind of thoughtfully designed around",
  "77:02": "what kind of what kind of change they",
  "77:05": "wanted to effect in a Debra Raj he gave",
  "77:07": "a great talk on that that all I'll share",
  "77:09": "the link to she did the follow up work",
  "77:12": "with joy and all I'll talk more about",
  "77:17": "kind of the diversity aspect next time",
  "77:20": "but so kind of in closing although we'll",
  "77:23": "return to this idea of kind of what what",
  "77:25": "practices can can we be doing to kind of",
  "77:27": "to improve our companies our work but",
  "77:30": "here are some suggestions for getting",
  "77:32": "started so just even analyzing a project",
  "77:35": "to see try creating a day",
  "77:37": "about about a data set that you're",
  "77:39": "working with the suggestion earlier that",
  "77:43": "I gave Christian lumps tutorial on the",
  "77:46": "compass recidivism algorithm where she",
  "77:47": "partnered with a public defender and an",
  "77:49": "innocent man who couldn't afford bail to",
  "77:52": "work with domain experts and people",
  "77:54": "impacted increasing diversity in your",
  "77:57": "workplace and then I'll say listen -",
  "77:59": "just to kind of be on the ongoing look",
  "78:01": "out for bias because it's not something",
  "78:02": "where you're like oh okay I've checked",
  "78:04": "for it I'm done I don't have to worry",
  "78:06": "about it but it's kind of an ongoing",
  "78:08": "ongoing issue well thank you it's 8",
  "78:13": "o'clock"
}