{
  "00:00": "so yeah so we're we're picking up here",
  "00:01": "is privacy and surveillance which we did",
  "00:03": "not finish last week there's a lot of",
  "00:07": "material and even more stuff happened in",
  "00:09": "the past week related to it and so I",
  "00:12": "just kind of briefly wanted to kind of",
  "00:14": "review where we were at and the first is",
  "00:18": "some issues around kind of what can go",
  "00:20": "wrong Dana will be used for other",
  "00:22": "purposes and so this link",
  "00:24": "Amulya posted in the forums about Grindr",
  "00:28": "being owned by a Chinese firm which has",
  "00:31": "created concerns around u.s. national",
  "00:34": "security of could could this be used for",
  "00:37": "for China to get data about military or",
  "00:40": "foreign officials on their sexual",
  "00:42": "preferences or even just location",
  "00:44": "tracking and this is something that",
  "00:46": "couldn't have been foreseen I you know",
  "00:49": "it wasn't obvious that it would happen",
  "00:50": "say five or ten years ago that it would",
  "00:53": "end up being bought by a Chinese company",
  "00:55": "so companies can change can change hands",
  "00:59": "another story from the last last week is",
  "01:02": "that Clearview AI which is the",
  "01:04": "incredibly controversial facial",
  "01:08": "recognition software that kind of",
  "01:11": "violated the terms of service for most",
  "01:13": "major companies to scrape faces from",
  "01:15": "everywhere its entire client list was",
  "01:18": "stolen in a data breach and has since",
  "01:20": "been published and includes a variety of",
  "01:25": "governments also lots of companies I",
  "01:27": "think it's remember definitely over a",
  "01:30": "thousand customers and then also raised",
  "01:33": "security concerns about just the fact",
  "01:36": "that they had this data breach and then",
  "01:38": "there was a separate article forget some",
  "01:42": "journalists were able to access some of",
  "01:44": "their kind of unsecured",
  "01:45": "app on on Amazon s3 I believe and so",
  "01:49": "they didn't have a client login but they",
  "01:51": "could still tell certain things about",
  "01:53": "the app so that's a news story last week",
  "01:57": "I shared about the story about how Saudi",
  "01:59": "Arabia infiltrated Twitter and this was",
  "02:01": "with two twitter employees who kind of",
  "02:06": "ended up being bought by the Saudi",
  "02:09": "Arabian government to pass information",
  "02:10": "the article though mentioned",
  "02:14": "someone on Twitter's team set our",
  "02:16": "formerly on Twitter's team mentioned how",
  "02:18": "it was very common to be approached by",
  "02:20": "governments including the US the CIA the",
  "02:24": "UK government so many governments had",
  "02:27": "approached them so this is another thing",
  "02:28": "that can happen with your data either",
  "02:31": "employees can be secretly sharing it or",
  "02:33": "governments can request request access",
  "02:35": "and in some cases get it and then we",
  "02:39": "also saw the example of police officers",
  "02:42": "widely abusing confidential databases in",
  "02:46": "many cases to stock former romantic",
  "02:48": "partners or to harass people who were",
  "02:51": "protesting police brutality and a common",
  "02:59": "kind of pattern with surveillance is",
  "03:01": "that it disproportionately harms",
  "03:02": "marginalised groups we saw this with",
  "03:06": "India's intrusive biometric ID forcing",
  "03:09": "HIV patients to forgo treatment and so",
  "03:12": "this was an article that interviewed a",
  "03:13": "number of people that were HIV positive",
  "03:15": "and had successfully been getting",
  "03:18": "antiretroviral treatment and then",
  "03:20": "stopped because they were worried about",
  "03:22": "being outed as being HIV positive or sex",
  "03:26": "workers or gay treating privacy for",
  "03:31": "survival is another tax on the poor this",
  "03:33": "is one of the this is one of the",
  "03:35": "articles I linked to last week and then",
  "03:38": "the fact that several states are",
  "03:41": "requiring prisoners to give up their",
  "03:43": "voice biometric ID in order to even be",
  "03:46": "allowed to make phone calls we also saw",
  "03:52": "that there is little evidence that",
  "03:53": "surveillance makes us safer so San",
  "03:57": "Diego's massive seven year experiment",
  "03:59": "with facial recognition technology",
  "04:01": "appears to be a flop this was something",
  "04:03": "in which I think 65,000 images were",
  "04:05": "collected and there was no proof that it",
  "04:06": "led to led to any arrests the",
  "04:11": "investigation with interviews with 40",
  "04:14": "different police departments using ring",
  "04:15": "which again found little evidence that",
  "04:18": "it's effective and then even kind of",
  "04:24": "some of the companies selling facial",
  "04:26": "recognition to",
  "04:27": "schools are admitting that it's not",
  "04:29": "going to stop stop school shootings and",
  "04:33": "then also surveillance is used to",
  "04:36": "suppress dissent and this has been shown",
  "04:37": "kind of throughout history and we have",
  "04:39": "examples happening in Hong Kong and then",
  "04:42": "also in the US and so this is very very",
  "04:47": "concerning so kind of in summary harms",
  "04:50": "the already marginalized data can be",
  "04:52": "used for other purposes whether that's",
  "04:54": "government's requesting it data breaches",
  "04:56": "sale of the company airs and data and it",
  "05:00": "stops dissent which is crucial for",
  "05:01": "social progress and that's something",
  "05:06": "that Ali pointed out last week is that",
  "05:09": "there are concerns either way so when",
  "05:12": "the system is not working properly so",
  "05:15": "the evidence that these products don't",
  "05:18": "even work or the data being full of",
  "05:19": "errors however like if you came to me",
  "05:23": "and had a new an updated study that now",
  "05:25": "ring is really effective at fighting",
  "05:28": "crime I would still think that the risk",
  "05:31": "outweigh the the positives and so my I",
  "05:35": "should say my hesitation is not just",
  "05:37": "that this doesn't work but around these",
  "05:39": "other issues and so I think it's",
  "05:40": "something that it's also bad when the",
  "05:42": "systems do work as intended because many",
  "05:44": "of the bad consequences are are about",
  "05:47": "the system kind of working as intended",
  "05:49": "and I thought the the line between these",
  "05:52": "is a bit blurry some of the examples I",
  "05:54": "wasn't sure exactly is this the system",
  "05:56": "working as intended or not you know for",
  "06:00": "instance like government collecting data",
  "06:02": "from the from a company is that kind of",
  "06:07": "the intention or not since it is such a",
  "06:09": "kind of common pattern and seems built",
  "06:11": "in so I just wanted to note this though",
  "06:14": "that it says quickly for me it's not",
  "06:17": "just that it doesn't work currently but",
  "06:19": "that there are a lot of harms even when",
  "06:21": "the system does work as intended any",
  "06:25": "more thoughts on on this",
  "06:30": "okay so oh and then I also shared the",
  "06:35": "examples this is from two weeks ago of",
  "06:38": "computer vision researchers saying that",
  "06:40": "they were giving up or considering",
  "06:43": "giving up doing computer vision research",
  "06:45": "just because the the negative",
  "06:48": "applications the military applications",
  "06:49": "and privacy concerns we're too hard to",
  "06:52": "ignore and I thought that would be a",
  "06:54": "good segue into discussing the LinkedIn",
  "06:56": "winner's article do artifacts have",
  "06:58": "politics and so this was in the reading",
  "07:02": "believe two weeks ago",
  "07:05": "and we didn't have a chance to discuss",
  "07:07": "it then but this idea of kind of rather",
  "07:10": "than thinking of technology as a neutral",
  "07:12": "tool and it's just about how the the",
  "07:14": "person using it you towards what purpose",
  "07:17": "they use it but is there something about",
  "07:20": "particular technologies that lend",
  "07:21": "themselves to certain uses and actually",
  "07:25": "I thought kind of came up and if you saw",
  "07:27": "I think this was also in the last week",
  "07:29": "the quote from the whatsapp founder",
  "07:31": "explicitly saying like this is just",
  "07:34": "technology doesn't have morals it's just",
  "07:36": "how people use it there's a lot of",
  "07:38": "discussion about that that quote on",
  "07:40": "Twitter so I wanted to ask about kind of",
  "07:43": "your thoughts both on this article and",
  "07:45": "the kind of the more general concept of",
  "07:46": "of whether whether technologies are",
  "07:50": "neutral or inclined towards toward",
  "07:52": "certain uses and sort and certain",
  "07:54": "redistributions of power yeah then the",
  "07:58": "the example the factory example really",
  "08:00": "struck me because it was kind of",
  "08:02": "presented under this veneer of like oh",
  "08:04": "this is about efficiency we're getting",
  "08:05": "these machines when really it was the",
  "08:07": "machines were less efficient than the",
  "08:10": "the workers but it was a valid about",
  "08:12": "unionization or blocking it and I think",
  "08:17": "of the zeynep defect see article from",
  "08:19": "this week kind of talks about that of",
  "08:21": "how you know initially a lot of these",
  "08:23": "technologies were seen as really kind of",
  "08:25": "democratizing or liberating forces and",
  "08:27": "then as people in power learned how to",
  "08:30": "utilize them towards their ends we're",
  "08:32": "seeing kind of the opposite where",
  "08:34": "they're becoming kind of more oppressive",
  "08:35": "forces",
  "08:41": "when reading this I like thinking about",
  "08:45": "like the early days of penny yeah",
  "08:49": "especially with all the conversation",
  "08:51": "about design and is that even if you",
  "08:57": "imagine that if fancy starts on a",
  "08:59": "certain direction the areas the",
  "09:01": "populations and the people who built it",
  "09:04": "like infrastructure or baseline design",
  "09:10": "decisions so in some ways it seems like",
  "09:13": "the whole scaffolding of suit themselves",
  "09:21": "better to be in the environments that",
  "09:24": "they were Melton a simple example could",
  "09:26": "be GPS or a travel app navigation apps",
  "09:31": "were built in cities to support cities",
  "09:34": "and you can imagine that like they",
  "09:38": "didn't work very well the early versions",
  "09:40": "in place like India which were not",
  "09:42": "structured in Japan where it didn't",
  "09:46": "stutter and required a lot of",
  "09:48": "modification in India I think in both",
  "09:51": "directions where cities we started",
  "09:53": "having defined names because people were",
  "09:57": "starting to use that's an interesting",
  "10:02": "point yeah politics I'm sure there's",
  "10:37": "much better studies and like anything I",
  "10:40": "did before but I think like inferring",
  "10:43": "the thing about heart or game uses",
  "10:48": "artifacts do change how things get use",
  "10:51": "and structure that's what you have the",
  "11:06": "rest of the context to be able to uh",
  "11:09": "yeah I think you know that's really",
  "11:11": "helpful to hear about it is about in",
  "11:13": "from archaeology the first one is that",
  "11:22": "in philosophy there's an area called",
  "11:25": "social ontology and so ontology is the",
  "11:28": "being or existence of something and so",
  "11:31": "social ontology is the study of how we",
  "11:34": "as social creatures create meaning out",
  "11:37": "of the objects world I think most social",
  "11:41": "oncologists would say that all objects",
  "11:43": "have some sort of strong meaning so",
  "11:50": "that's just comment and the second one",
  "11:52": "is that whenever you're dealing with an",
  "11:54": "object that or a system that involves",
  "11:58": "corporate profit I think that to look",
  "12:01": "closely really you know what its",
  "12:07": "intentions are well thank you we are you",
  "12:22": "getting right yes yeah yeah and we'll",
  "12:30": "talk even more about that and yeah the",
  "12:32": "ecosystem stuff in the second half of",
  "12:34": "tonight oh thank you",
  "12:37": "and this is a I should say this is",
  "12:39": "something that people kind of disagree",
  "12:41": "about then I in the interest of time I",
  "12:46": "think will not spend too long on the",
  "12:47": "Philip rogue away paper but I really I",
  "12:50": "really liked it and just a few a few",
  "12:52": "points that he makes are that",
  "12:55": "surveillance is an instrument of power",
  "12:57": "mass surveillance tends to produce",
  "13:00": "uniform come",
  "13:01": "client and shallow people privacy is a",
  "13:03": "social good which I'll talk more about",
  "13:05": "in a moment and that creeping",
  "13:07": "surveillance is hard to stop due to",
  "13:09": "interlocking corporate and government",
  "13:10": "interest and something I learned from",
  "13:13": "the paper that I didn't know before is",
  "13:14": "that Eisenhower which originally talked",
  "13:16": "about the military-industrial academic",
  "13:18": "complex in a draft of his speech but",
  "13:21": "then the final speech he just said the",
  "13:23": "military-industrial complex but I",
  "13:25": "thought that was interesting that",
  "13:26": "academia was originally in there as well",
  "13:32": "well doing a photo no yes and now I want",
  "13:42": "to talk about some kind of steps towards",
  "13:44": "solutions and I'm gonna first go through",
  "13:47": "some proposals that I do not think will",
  "13:49": "solve the problem but that come up",
  "13:51": "frequently an example of what motivates",
  "13:55": "companies to change some hope from",
  "13:57": "history the idea of privacy as a public",
  "14:00": "good and then some kind of the specific",
  "14:02": "use cases around regulating data",
  "14:04": "collection and political ads so an idea",
  "14:09": "that comes up a lot is whether we should",
  "14:11": "pay people for their data and most",
  "14:13": "recently Andrew yang announced on",
  "14:16": "Twitter last week that he is going to be",
  "14:18": "discussing this with Kara Swisher at",
  "14:20": "South by Southwest and this was kind of",
  "14:23": "one of his policy proposals and sorry I",
  "14:26": "disagree I think I see it's coming from",
  "14:28": "a good place this idea of wanting",
  "14:30": "wanting people to be compensated because",
  "14:32": "kind of their data is allowing companies",
  "14:35": "to make all this money but I disagree",
  "14:37": "with this kind of one reason is this",
  "14:42": "fails to treat privacy as a public good",
  "14:44": "and so and we saw this in kind of the",
  "14:48": "message cyclists key article last week",
  "14:51": "on kind of making these analogies with",
  "14:53": "the environment of when we had kind of",
  "14:56": "rivers catching on fire because we're so",
  "14:58": "heavily polluted or terrible smog you",
  "15:02": "know this those problems couldn't be",
  "15:05": "solved by kind of companies paying",
  "15:08": "individuals for for how they are being",
  "15:10": "impacted or letting individuals decide",
  "15:12": "like am I",
  "15:14": "okay personally with it with the company",
  "15:16": "dumping dumping this waste in the river",
  "15:18": "nam but that you needed a kind of more",
  "15:20": "collective collective response and to to",
  "15:24": "start kind of reframing privacy as as a",
  "15:28": "public good and so message referred to",
  "15:30": "it as ambient privacy which actually I",
  "15:34": "think have a slide on later the other is",
  "15:38": "that it feels to treat privacy as a",
  "15:39": "human right and so I think that there's",
  "15:41": "a concerning precedent around the idea",
  "15:43": "of kind of paying money and and this is",
  "15:48": "hard because I think how we classify and",
  "15:52": "think about privacy is still being",
  "15:53": "framed",
  "15:54": "and so one article referred to privacy",
  "15:58": "is something that emanates from human",
  "15:59": "rights which I like so even if it's not",
  "16:01": "officially a human right yet but",
  "16:04": "realizing that it's at least kind of",
  "16:06": "related to them that when we start",
  "16:09": "putting a monetary value on that that",
  "16:11": "that is kind of not a great direction to",
  "16:13": "go in because it legitimizes the idea of",
  "16:15": "kind of it not being essential and also",
  "16:19": "it kind of increases the the class",
  "16:22": "issues in which poor people may have no",
  "16:26": "choice but really feel compelled to give",
  "16:28": "up their data for money also you know",
  "16:30": "this could flip and become a scenario",
  "16:32": "more where people are paying to try to",
  "16:34": "get some sort of privacy as opposed to",
  "16:37": "being paid and kind of will exacerbate",
  "16:38": "the the class difference that we already",
  "16:41": "see it's very virtually impossible for",
  "16:46": "individuals to calculate the value of",
  "16:48": "their data this is something that's",
  "16:50": "spread of spread over time and really",
  "16:52": "changes in aggregation and it's hard as",
  "16:54": "an individual to know you know what will",
  "16:56": "happen with your data when it's",
  "16:57": "aggregated with the data of other people",
  "16:59": "and with data from other sources cryptic",
  "17:04": "Ally on Twitter who's that I will I will",
  "17:06": "leave now had a great thread on this",
  "17:08": "highlighting how this really puts the",
  "17:09": "burden of time and education on the",
  "17:11": "consumer not on the firm's that have all",
  "17:14": "the power and then Arvind Rania and also",
  "17:18": "had a great thread on this thing that",
  "17:21": "this would entrench the asymmetric and",
  "17:22": "exploitative relationship between firms",
  "17:24": "and individuals",
  "17:26": "so this is kind of my take on this and",
  "17:29": "not just my take I'm kind of many many",
  "17:31": "other privacy experts have spoken out",
  "17:33": "about this there are thoughts on kind of",
  "17:36": "this proposal or I will say that there's",
  "17:39": "even if an individual's data is not out",
  "17:42": "there there are things that I think we",
  "17:43": "lose as a society when we lose kind of a",
  "17:46": "broader sense of privacy yeah I guess",
  "17:50": "the experience that you were there's",
  "17:56": "like infinite states if you got arrested",
  "17:58": "your mug shot and other informations if",
  "18:02": "I cook online and sometimes like their",
  "18:05": "party companies and organizations will",
  "18:08": "like all think basically we host your",
  "18:09": "photo and information about your arrest",
  "18:11": "notice that before you've been tried",
  "18:13": "before you've been charged before you've",
  "18:15": "been convicted and like it's cause it's",
  "18:18": "like create like this entire industry",
  "18:20": "around like sort of like tying you to",
  "18:27": "some alleged crime that maybe you've",
  "18:29": "been found innocent of then you have to",
  "18:32": "pay like $30 it is and like $30 we may",
  "18:37": "not be a ton of money",
  "18:38": "the practical times but a lot of people",
  "18:40": "when they find out about this find it",
  "18:42": "really impulsive like personal sort of",
  "18:45": "like principal level and I think that's",
  "18:47": "sort of what goes back to this idea that",
  "18:49": "like it's not a finally it's not a",
  "18:51": "property right issue do you like it it's",
  "18:55": "not a reasonable or fair that like",
  "18:56": "somebody can take information and sort",
  "19:00": "of like coerce us into paying for that",
  "19:02": "information to be conforms to our to our",
  "19:06": "consent and the cabin information is",
  "19:09": "treated up some people arrested or",
  "19:11": "whatever but again to see it is a he or",
  "19:17": "we tend to think oh and briefly wanted",
  "19:25": "to say so differential privacy which",
  "19:27": "came up last week although not my name",
  "19:30": "under when we're talking about the",
  "19:32": "census and kind of you know what are",
  "19:34": "ways to kind of help and I think I think",
  "19:36": "that is a use case where differential",
  "19:37": "privacy can be useful I do think",
  "19:39": "differential privacy is off",
  "19:40": "overhyped and so I share some of the",
  "19:42": "critiques that that rogue away included",
  "19:46": "and in his paper that it often",
  "19:48": "implicitly assumes that the database",
  "19:50": "owner is the kind of the good guy and",
  "19:52": "that you're protecting the data from",
  "19:54": "others",
  "19:55": "I need does share a kind of rebuttal to",
  "19:58": "this of kind of more decentralized",
  "20:00": "designs although I think that kind of",
  "20:05": "default is a more centralized version",
  "20:08": "and so you know as we saw kind of with",
  "20:10": "some of the previous examples that is",
  "20:11": "often not necessarily the case it's",
  "20:14": "still kind of framing harm is something",
  "20:16": "that's individual and not necessarily",
  "20:18": "community-wide and rarely considers the",
  "20:22": "alternative of collecting less data and",
  "20:23": "so I think that's kind of a key thing",
  "20:25": "that it's important I think sometimes",
  "20:28": "technical fixes can be very appealing",
  "20:30": "but I think it's really important to",
  "20:32": "consider the kind of less technical",
  "20:33": "could we just collect less data and it",
  "20:37": "also gives corporations potentially a",
  "20:39": "means for whitewashing the risk and so",
  "20:41": "this is not to say that differential",
  "20:42": "privacy is never the answer but that I",
  "20:44": "think it can be overhyped as an answer",
  "20:46": "and I do share these kind of concerns",
  "20:49": "about when it can be misused okay so now",
  "20:54": "on towards towards solutions and so",
  "20:57": "first I'm going to share an example of",
  "20:59": "what motivated a company to do something",
  "21:02": "differently this starts very starkly",
  "21:06": "with Facebook so you know the UN a UN",
  "21:10": "has found that Facebook played a",
  "21:12": "determining role in the Myanmar Myanmar",
  "21:14": "genocide of the row hinga one of the",
  "21:18": "best articles I've read on it is from",
  "21:20": "Timothy McLaughlin and wired and he",
  "21:23": "interviewed people that warned Facebook",
  "21:26": "execs in 2013 and 2014 and 2015 about",
  "21:30": "how the platform is being used to incite",
  "21:31": "violence and the person that warned them",
  "21:35": "in 2015 even said that there was the",
  "21:37": "potential for Facebook to play the role",
  "21:40": "in Myanmar that the radio broadcast",
  "21:41": "played during the Rwandan genocide and",
  "21:44": "yet as of 2015 Facebook only had four",
  "21:47": "contractors that spoke Burmese on staff",
  "21:50": "which is just wild",
  "21:53": "it's the",
  "21:54": "terrible that they really did not take",
  "21:55": "significant action and someone said in",
  "21:58": "the article this is not 20/20 hindsight",
  "21:59": "the scale of this problem was",
  "22:01": "significant and it was already apparent",
  "22:04": "and so this is yeah kind of very tragic",
  "22:07": "and it's just really difficult to read",
  "22:09": "kind of how little action Facebook took",
  "22:15": "so then this might have been 2018 when",
  "22:20": "when Zuckerberg was testifying before",
  "22:22": "Congress and he said okay now we're",
  "22:24": "gonna hire dozens of Burmese language",
  "22:26": "content reviewers to try to address this",
  "22:28": "so in contrast Germany passed a stricter",
  "22:34": "law about hate speech called nets D D G",
  "22:36": "and Facebook hired 1200 people in under",
  "22:41": "a year and so the difference here is",
  "22:44": "that yes that Germany was if Facebook",
  "22:49": "violated the sloth they could have been",
  "22:51": "fined I think I was around 50 million",
  "22:53": "euros so a very significant number and",
  "22:56": "so yes if someone said money is the",
  "22:59": "difference between these examples and so",
  "23:01": "I'm sharing this not to say that kind of",
  "23:03": "the particular Durbin law as a model but",
  "23:05": "just this contrast between kind of being",
  "23:08": "told that you are contributing to an",
  "23:10": "actual genocide versus facing a very",
  "23:13": "hefty penalty and so that shows kind of",
  "23:16": "this is something that got Facebook to",
  "23:18": "take action when they thought there",
  "23:20": "would be a substantial kind of penalty",
  "23:22": "and it's important that the penalty is",
  "23:24": "not just a cost of doing business fine",
  "23:26": "which many end up being but it has to be",
  "23:29": "significant and a credible threat that",
  "23:32": "it's likely to happen and so I always",
  "23:34": "kind of think about this as an example",
  "23:36": "of legislation and the threat of",
  "23:40": "credible and significant financial",
  "23:42": "penalties making an impact yes yeah",
  "23:49": "because gdpr is something that and and",
  "23:52": "some of this is I think there is",
  "23:54": "definitely more up to see kind of how",
  "23:56": "strictly gdpr is enforced and in what",
  "23:59": "cases but yeah that kind of credible",
  "24:00": "threat of a significant penalty can can",
  "24:04": "motivate companies in a way that",
  "24:05": "nothing else does yeah so I wanted to",
  "24:09": "kind of share that I'm some some hope",
  "24:12": "from history",
  "24:13": "so I find I mean I think the problems",
  "24:15": "were facing in many areas are pretty",
  "24:19": "overwhelming and complex and so I know",
  "24:22": "that can feel kind of discouraging of",
  "24:25": "just how can we even tackle this when it",
  "24:26": "seems so complex and so I think it's",
  "24:28": "helpful to remember kind of previous",
  "24:31": "successes but many of which I kind of",
  "24:33": "now take for granted something I really",
  "24:35": "liked about data sheets for data sets",
  "24:37": "which was assigned reading and week two",
  "24:39": "is that they covered three case studies",
  "24:42": "of how standardization and regular as a",
  "24:45": "regulation came to different industries",
  "24:47": "and so one in particular I'll talk about",
  "24:51": "is car safety I also listen to a 99%",
  "24:55": "invisible episode on this that was",
  "24:56": "really fascinating this is a design",
  "24:58": "podcast but early cars had sharp metal",
  "25:02": "knobs on the dashboard that was lodged",
  "25:04": "in people's skulls during crashes non",
  "25:08": "collapsible steering columns would",
  "25:09": "frequently impale drivers and the",
  "25:12": "collapsible steering column was invented",
  "25:14": "but was not implemented for many many",
  "25:16": "years because there was no financial",
  "25:18": "reason to implement it but it's said",
  "25:21": "that the collapsible steering column has",
  "25:23": "saved more lives than related to car",
  "25:26": "safety than anything other than",
  "25:28": "seatbelts there's also this widespread",
  "25:32": "belief that cars were dangerous because",
  "25:34": "of the people driving them and so for",
  "25:36": "really for decades the kind of",
  "25:38": "prevailing sentiment was like you know",
  "25:40": "cars are just the way they are this is",
  "25:42": "how cars are the problem is when we have",
  "25:44": "bad people driving them and so there's",
  "25:47": "nothing we can do there was also the",
  "25:50": "glass they used kind of regular glass",
  "25:52": "that would shatter in very dangerous",
  "25:54": "ways and the car companies were very",
  "25:57": "resistant to people even discussing car",
  "25:59": "safety because they didn't want people",
  "26:00": "you know if people start thinking about",
  "26:02": "car safety they're gonna think about",
  "26:03": "death and accidents and so they really",
  "26:06": "tried to stifle that discussion and kind",
  "26:08": "of advocates and activists had to work",
  "26:10": "for decades to even change the",
  "26:12": "conversation around this GM hired",
  "26:15": "private detectives to shadow Ralph Nader",
  "26:17": "and try to dig up dirt on him",
  "26:19": "discredit him so it was really and I did",
  "26:22": "not know most of this history is",
  "26:24": "something that people worked very hard",
  "26:25": "and now I mean while there are many",
  "26:29": "problems with car culture it is",
  "26:31": "something at least where car companies",
  "26:32": "have acknowledged they can change their",
  "26:34": "designs they even brag about safety as a",
  "26:37": "feature and that is kind of drastically",
  "26:39": "different than the the situation a few",
  "26:40": "decades ago Claudia's got her hand up oh",
  "26:46": "you do or don't okay can you pass the",
  "26:49": "ketchup ox back I'll say one more thing",
  "26:51": "about cars while you're passing it back",
  "26:53": "it was only in 2011 that they may",
  "26:57": "undetected to also represent the average",
  "26:59": "woman's body and not just be",
  "27:01": "representing men and again that's 2011",
  "27:04": "so relatively recently um I think that's",
  "27:06": "also just kind of a very concrete",
  "27:08": "example of kind of the dif difference",
  "27:12": "that the regulation can make I mean",
  "27:15": "something and I don't know what the more",
  "27:16": "recent statistics are but up until that",
  "27:18": "point women were 40% more likely to be",
  "27:21": "injured in a car crash of the same",
  "27:23": "impact compared to a man and kind of",
  "27:26": "very likely due to these differences in",
  "27:27": "testing all right Claudia",
  "27:38": "it just seems so much slower then the",
  "27:42": "rate at which technology yeah you know",
  "27:52": "things have changed dramatically so you",
  "27:56": "know I don't know how to for them to",
  "27:58": "implement seatbelts or car seats for",
  "28:02": "kids but I don't know yeah I know and",
  "28:09": "that is and that is a valid point this",
  "28:10": "was something that took a while",
  "28:12": "technology is rapidly rapidly evolving",
  "28:17": "and it is I mean there are other ways",
  "28:19": "that kind of the the parallels for the",
  "28:22": "other other interesting point they make",
  "28:24": "is just beginning to even collect the",
  "28:27": "kind of data on car crashes was kind of",
  "28:29": "a key key victory to even kind of have",
  "28:31": "that day",
  "28:34": "well I don't even think we've had well",
  "28:38": "yeah we'd have the car crashes data wise",
  "28:42": "but I feel like policymakers don't even",
  "28:46": "get the internet yet so how are they",
  "28:51": "going to get a grip on this in a timely",
  "28:54": "manner yeah and that is a concern about",
  "28:58": "a timely manner another analogy I'm",
  "29:02": "Julia Angwin who was a senior reporter",
  "29:04": "at Pro Publica and now is editor in",
  "29:06": "chief of the markup and I'm gonna ship",
  "29:09": "link to an interview she gave in a few",
  "29:12": "slides but one thing she compared it to",
  "29:14": "the Industrial Revolution and talked",
  "29:16": "about you know with the Industrial",
  "29:17": "Revolution we had a few decades of you",
  "29:21": "know children working in factories",
  "29:22": "12-hour days and incredibly unsafe",
  "29:24": "conditions and she talks about how it",
  "29:27": "just took a while to even kind of gather",
  "29:30": "the language and be able to describe the",
  "29:31": "problem and then journalists kind of had",
  "29:33": "to do a lot of this just even covering",
  "29:36": "what is the problem and how do we talk",
  "29:37": "about this and that you know and then",
  "29:39": "the kind of from there that helps for",
  "29:42": "kind of advocates for organizing around",
  "29:44": "this and activists and organizing but",
  "29:47": "that she says we're kind of still in",
  "29:48": "this phase of just how do we even kind",
  "29:50": "of talk about and describe the problems",
  "29:51": "we're facing which I found both kind of",
  "29:56": "reassuring that it's like okay it's okay",
  "29:58": "that you know we don't have like the",
  "30:00": "this is the exact solution to implement",
  "30:02": "but that we do need to kind of just even",
  "30:04": "talk about the problems and kind of",
  "30:05": "build up our language and understanding",
  "30:07": "of them and then I'll say I'm data",
  "30:14": "sheets for data sets also talked about",
  "30:16": "the pharmaceutical industry and the",
  "30:19": "industry for kind of electronic",
  "30:21": "components like circuits and resistors",
  "30:23": "and transistors so then there is the",
  "30:28": "example from Missy to Klaus keys post",
  "30:31": "that I mentioned earlier of you know",
  "30:34": "just kind of pollution and and again",
  "30:37": "this is an area where it can be",
  "30:39": "discouraging because we are still facing",
  "30:40": "very significant environmental issues",
  "30:42": "but to look back at some of the",
  "30:44": "environmental wins",
  "30:45": "that we that we've had can be helpful to",
  "30:47": "just kind of remember that we have we",
  "30:50": "have made some progress on this issue",
  "30:51": "and kind of what those what those wins",
  "30:53": "can look like he wrote the",
  "30:59": "infrastructure of mass surveillance is",
  "31:01": "too complex and the tech oligopoly too",
  "31:03": "powerful to make it meaningful to talk",
  "31:05": "about individual consent to what extent",
  "31:09": "is living in a surveillance saturated",
  "31:11": "world compatible with pluralism and",
  "31:13": "democracy which is also kind of a I",
  "31:16": "think a big question to consider and get",
  "31:18": "that kind of what we were talking about",
  "31:19": "earlier with public goods but this",
  "31:21": "notion that kind of ambient privacy and",
  "31:24": "having a society where not all our",
  "31:27": "interactions are on the record can have",
  "31:29": "kind of positive positive impacts and I",
  "31:32": "think there I think there are a lot of",
  "31:33": "kind of helpful analogies to me made",
  "31:36": "there what kind of the environment in",
  "31:42": "sorry I'll get to slightly more positive",
  "31:43": "framing in a moment",
  "31:45": "now this is the the towards solutions",
  "31:46": "section but I do I do really think it's",
  "31:49": "helpful to try to kind of learn from",
  "31:50": "history and then look at it victories in",
  "31:53": "history so this I really liked this this",
  "31:58": "was an interview with Julia Angwin and",
  "31:59": "Trevor Paglen and they kind of argue",
  "32:02": "that even privacy they think it's not",
  "32:03": "the right right framing for what we're",
  "32:06": "talking about and so I think this quote",
  "32:09": "was from Trevor who's an artist but he",
  "32:12": "refers to anonymity as a public resource",
  "32:14": "and so kind of thinking about says there",
  "32:18": "you know there are a lot of kind of",
  "32:19": "de-facto rights and liberties that arise",
  "32:21": "from not having every single action in",
  "32:23": "your everyday life having economic and",
  "32:25": "political consequences and that's",
  "32:27": "something that kind of we've been",
  "32:29": "drawing from and not necessarily",
  "32:30": "realizing the the benefit of and Julia",
  "32:35": "said I really like this framing privacy",
  "32:37": "is not about wanting to be alone it's",
  "32:39": "about wanting to be able to participate",
  "32:40": "in the wonderful connectedness that the",
  "32:42": "Internet has brought to the world but",
  "32:44": "without giving everything up kind of",
  "32:46": "recognizing that there are you know is",
  "32:48": "something wonderful about being",
  "32:49": "connected and the privacy is not an",
  "32:52": "anti-social thing but it's about kind of",
  "32:54": "wanting to enjoy some of these benefits",
  "32:56": "without having to sacrifice so much",
  "33:00": "okay so now a kind of a hopeful hopeful",
  "33:03": "story so this is tuwana petit who gave",
  "33:07": "one of the keynotes at our tech policy",
  "33:09": "workshop and she's a director of digital",
  "33:13": "justice for the Detroit tech community",
  "33:15": "or Detroit community tech project and so",
  "33:20": "in Detroit and I think we talked about",
  "33:22": "this last time has this project",
  "33:23": "greenlight that's putting kind of",
  "33:25": "surveillance cameras all over the place",
  "33:27": "and she talked about this program",
  "33:30": "they're doing called green chairs",
  "33:31": "instead of green lights and the idea is",
  "33:34": "to give people free chairs if they'll",
  "33:36": "agree to sit on their porches more and",
  "33:39": "talk to their neighbors and this",
  "33:41": "actually started believe in the 80s in",
  "33:44": "Michigan and they're kind of people were",
  "33:48": "worried about like the safety safety of",
  "33:50": "children walking home from school and so",
  "33:52": "they did this community project of let's",
  "33:53": "have more people sitting outside during",
  "33:55": "the hours that children are walking home",
  "33:57": "from school and get people talking to",
  "33:59": "their neighbors more again and that was",
  "34:02": "kind of successful honest I really liked",
  "34:04": "this and so this is a very like low-tech",
  "34:06": "solution and she contrasted oh it's",
  "34:12": "gonna thought it quoted it I wrote about",
  "34:13": "this and I linked to a source about in a",
  "34:15": "blog post I wrote last month but she",
  "34:18": "said that surveillance is not safety and",
  "34:20": "so kind of making this distinction",
  "34:22": "between kind of surveillance and more",
  "34:24": "cameras and kind of what actually makes",
  "34:27": "people feel safe and potentially having",
  "34:29": "more community and better relationships",
  "34:30": "with their neighbors can contribute to",
  "34:33": "safety roman Chowdhury something she",
  "34:37": "said at the conference that that stood",
  "34:39": "out to me was that how surveillance is",
  "34:42": "often kind of part of increasing",
  "34:44": "militarization and even though it's kind",
  "34:47": "of in the name of increasing safety we",
  "34:51": "think of heavily militarized societies",
  "34:53": "is pretty low trusts or societies and so",
  "34:56": "that the kind of the relationship",
  "34:57": "doesn't seem to hold in terms of",
  "34:59": "militarization doesn't actually make",
  "35:02": "people kind of safe or increased trust",
  "35:04": "in society",
  "35:09": "so okay now I'm gonna talk about some",
  "35:13": "kind of so this is kind of I like this",
  "35:14": "example of just thinking about you know",
  "35:16": "what are some kind of not necessarily",
  "35:18": "even involving technology solutions that",
  "35:20": "could address kind of the the same sort",
  "35:22": "of problems that that surveillance is",
  "35:24": "offering to address on to policy",
  "35:28": "proposals and so these are going to be",
  "35:30": "kind of a little bit more narrow in",
  "35:32": "their focus this was a op-ed in The New",
  "35:37": "York Times by the author of I think",
  "35:41": "since the anti social network is the",
  "35:43": "name of his book but he says the this is",
  "35:46": "seva divide high and high an eighth on",
  "35:49": "the key is to limit data collection and",
  "35:52": "the use of personal data to ferry ads",
  "35:53": "and other content to discrete segments",
  "35:56": "of Facebook users unfortunately that's",
  "35:58": "the core of Facebook's business model so",
  "36:01": "one concrete proposal that could",
  "36:04": "potentially work in the u.s. would be to",
  "36:06": "restrict the targeting of political ads",
  "36:08": "in any medium to the level of the",
  "36:10": "electoral electoral district of the race",
  "36:12": "so kind of not allowing you know this",
  "36:14": "extreme micro targeting but really",
  "36:16": "trying to keep it keep it broader other",
  "36:19": "app kind of proposals around",
  "36:21": "advertisements or you know to have them",
  "36:23": "based on the content of the page you're",
  "36:25": "looking at not so much on this",
  "36:27": "compilation of your personal data this",
  "36:31": "was a report that came from John Hopkins",
  "36:34": "and you'ens UNC and one of the authors",
  "36:37": "said there was even bipartisan agreement",
  "36:39": "on how to regulate digital ads in a",
  "36:41": "basic way similar to how TV ads are",
  "36:43": "governed more transparency databases of",
  "36:46": "content actual government oversight and",
  "36:49": "so this is something that we kind of had",
  "36:52": "for particularly for political ads on TV",
  "36:54": "that we don't yet have for for digital",
  "36:56": "ads and as zeynep defect she wrote an",
  "37:03": "article in 2018 and this is during the",
  "37:06": "Facebook hearings and she actually said",
  "37:08": "you know we don't need to interview mark",
  "37:10": "zuckerberg we already know more than",
  "37:11": "enough about facebook we can just look",
  "37:13": "at their actions for the last 10 years",
  "37:15": "and she proposed that data collection",
  "37:18": "should only be through clear concise and",
  "37:20": "often people should have access to all",
  "37:23": "data collected on them data collection",
  "37:26": "should be limited to specifically",
  "37:27": "enumerated purposes and I think this",
  "37:29": "also includes the time limit that came",
  "37:31": "up in previous a previous class but the",
  "37:34": "idea of you know when data is collected",
  "37:36": "it's we're gonna use it for this purpose",
  "37:37": "for this length of time and that's it as",
  "37:40": "opposed to right now where it's kind of",
  "37:41": "this you know indefinite they have it",
  "37:43": "forever I'm actually I'll pot oh",
  "37:50": "actually you work okay I will finish",
  "37:53": "this I think I just have like two slides",
  "37:55": "left and no it will have our break you",
  "37:57": "know we're running a little bit late so",
  "37:59": "quote I like to remember from Zeynep who",
  "38:02": "I really admire is what we need to fear",
  "38:05": "most is not what AI will do to us on its",
  "38:07": "own but how people in power will use AI",
  "38:10": "to control and manipulate us and so I",
  "38:12": "think that's a kind of important",
  "38:14": "principle to keep in mind when",
  "38:16": "considering how how can and should we",
  "38:19": "regulate and then these are kind of some",
  "38:23": "of the top experts I recommend following",
  "38:25": "on on issues of privacy and surveillance",
  "38:34": "and then with that we'll take we'll take",
  "38:37": "our seven minute break so let's meet",
  "38:39": "back at 7:15"
}